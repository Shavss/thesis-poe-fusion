{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Environmental expert (XGBoost): final HSI model (context expert)\n",
    "\n",
    "**Author:** Kacper Ryske  \n",
    "**Date:** January 2026  \n",
    "**Purpose:** Train and evaluate an environmental feature-based classifier whose calibrated probabilities serve as the contextual (HSI) expert in the late-fusion ensemble.\n",
    "\n",
    "This notebook trains the **final environmental (context) expert** used in the thesis: an XGBoost multi-class model that outputs **occurrence probabilities for 8 target insect species**, conditioned on weather, land-cover, temporal context, and related environmental features.\n",
    "\n",
    "## Validation strategy\n",
    "- **Train:** 80% of **2018â€“2024** observations (random stratified split by species)\n",
    "- **Validation:** 20% of **2018â€“2024** observations (random stratified split by species)\n",
    "- **Test:** **2025** observations (temporal holdout; not used in model selection)\n",
    "\n",
    "## Data layout (local, not tracked)\n",
    "- `data/amsterdam_2/train/observations_filtered_50m_accuracy.parquet`  (2018â€“2024)\n",
    "- `data/amsterdam_2/val/observations_filtered_50m_accuracy.parquet`    (2025)\n",
    "\n",
    "## Outputs (local, not tracked)\n",
    "- Model + metadata: `models/context/`\n",
    "- Predictions + metrics: `outputs/context_xgb/`\n",
    "\n",
    "> Tip: keep `data/`, `models/`, and `outputs/` in `.gitignore`.\n",
    "\n",
    "## How to run\n",
    "Run the notebook **top-to-bottom**. The precomputed parquet files are expected to include:\n",
    "- observation metadata (e.g., `species`, `observed_at`, `final_latitude`, `final_longitude`, â€¦)\n",
    "- weather features (e.g., `temp_c`, `rhum`, `wspd_ms`, `prcp_mm`, `cloud_cover`, `swrad`, `vpd_kpa`)\n",
    "- land-cover fractions / indices at multiple radii (e.g., `wc50_water`, `wc250_tree_cover`, â€¦)\n",
    "\n",
    "## What this notebook produces\n",
    "- Trained XGBoost model (context expert) + inference metadata (feature list, species mapping)\n",
    "- Test-set probabilities and summary metrics for downstream fusion\n",
    "\n",
    "## Workflow\n",
    "1. **Data loading & splitting**: load preprocessed observations and create train/val split; keep 2025 as temporal test\n",
    "2. **Feature engineering**: derive environmental features from contextual inputs\n",
    "3. **Feature selection**: remove identifiers/leakage features and known non-informative features\n",
    "4. **Model training**: train candidate XGBoost configurations with early stopping on the validation split\n",
    "5. **Evaluation**: report Top-k accuracy and probabilistic metrics (log loss, Brier)\n",
    "6. **Export**: save model, metadata, and test-set probabilities for fusion\n",
    "\n",
    "> Note: Probability calibration and diagnostic plots (reliability diagrams, confusion matrix, feature importance) are typically kept in a separate **analysis notebook** to keep this training notebook lightweight.\n",
    "\n",
    "## Target species\n",
    "1. *Aglais urticae* (Small Tortoiseshell)  \n",
    "2. *Apis mellifera* (Western Honey Bee)  \n",
    "3. *Bombus lapidarius* (Red-tailed Bumblebee)  \n",
    "4. *Bombus terrestris* (Buff-tailed Bumblebee)  \n",
    "5. *Coccinella septempunctata* (Seven-spot Ladybird)  \n",
    "6. *Episyrphus balteatus* (Marmalade Hoverfly)  \n",
    "7. *Eristalis tenax* (Drone Fly)  \n",
    "8. *Eupeodes corollae* (Footballer Hoverfly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Imports, plotting defaults, and random seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import (\n",
    "    classification_report, log_loss, accuracy_score,\n",
    ")\n",
    "\n",
    "# Optional plotting (comment out if you want a pure-training notebook)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Paths and output folders\n",
    "Repo-relative paths so the notebook runs on any machine (no absolute `/Users/...` paths).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PATHS (repo-friendly; no absolute /Users/... paths)\n",
    "# ============================================================================\n",
    "\n",
    "REPO_ROOT = Path.cwd().resolve()\n",
    "# If running from notebooks/, step up one level\n",
    "if not (REPO_ROOT / \"configs\").exists() and (REPO_ROOT.parent / \"configs\").exists():\n",
    "    REPO_ROOT = REPO_ROOT.parent\n",
    "\n",
    "DATA_ROOT = REPO_ROOT / \"data\" / \"amsterdam\"\n",
    "\n",
    "HISTORICAL_PATH = DATA_ROOT / \"train\" / \"observations_filtered_50m_accuracy.parquet\"  # 2018â€“2024\n",
    "TEST_PATH       = DATA_ROOT / \"val\"   / \"observations_filtered_50m_accuracy.parquet\"  # 2025 holdout\n",
    "\n",
    "MODEL_DIR = REPO_ROOT / \"models\" / \"context\"\n",
    "OUT_DIR   = REPO_ROOT / \"outputs\" / \"context_xgb\"\n",
    "SPECIES_MAP_PATH = MODEL_DIR / \"species_mapping_FINAL_no_vespula.csv\"\n",
    "\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"preds\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"HISTORICAL_PATH:\", HISTORICAL_PATH)\n",
    "print(\"TEST_PATH:\", TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Experiment configuration\n",
    "Target species and global constants.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TARGET_SPECIES = [\n",
    "    \"Apis mellifera\", \"Eristalis tenax\",\n",
    "    \"Bombus terrestris\", \"Coccinella septempunctata\",\n",
    "    \"Bombus lapidarius\", \"Episyrphus balteatus\",\n",
    "    \"Aglais urticae\", \"Eupeodes corollae\",\n",
    "]\n",
    "\n",
    "GRID_DECIMALS = 2  # used only for coarse bins (not used as model features)\n",
    "SCALE = 10 ** GRID_DECIMALS\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL XGBOOST HSI MODEL (PROPER TRAIN/VAL/TEST SPLIT)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Target species: {len(TARGET_SPECIES)}\")\n",
    "print(\"Validation: 80/20 stratified split (2018â€“2024) + 2025 temporal test\")\n",
    "print(\"Metrics: Top-K Accuracy + Log Loss + Brier Score\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Load and split data\n",
    "2018â€“2024 are split into train/val (stratified by species). The 2025 data are kept as a temporal holdout test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 1: LOAD AND SPLIT DATA\n",
    "# ============================================================================\n",
    "\n",
    "def add_coarse_bins(df: pd.DataFrame, grid_decimals: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"Add coarse spatial bins (lat_bin, lon_bin) from final_latitude/final_longitude.\n",
    "\n",
    "    These bins are for analysis only; coordinates are excluded from model features.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    scale = 10 ** grid_decimals\n",
    "\n",
    "    # robust parsing\n",
    "    if \"final_latitude\" in df.columns:\n",
    "        df[\"latitude\"] = pd.to_numeric(df[\"final_latitude\"], errors=\"coerce\")\n",
    "    elif \"latitude\" in df.columns:\n",
    "        df[\"latitude\"] = pd.to_numeric(df[\"latitude\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"latitude\"] = np.nan\n",
    "\n",
    "    if \"final_longitude\" in df.columns:\n",
    "        df[\"longitude\"] = pd.to_numeric(df[\"final_longitude\"], errors=\"coerce\")\n",
    "    elif \"longitude\" in df.columns:\n",
    "        df[\"longitude\"] = pd.to_numeric(df[\"longitude\"], errors=\"coerce\")\n",
    "    else:\n",
    "        df[\"longitude\"] = np.nan\n",
    "\n",
    "    df[\"lat_bin\"] = np.round(df[\"latitude\"] * scale).astype(\"Int64\")\n",
    "    df[\"lon_bin\"] = np.round(df[\"longitude\"] * scale).astype(\"Int64\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading historical data (2018â€“2024)...\")\n",
    "historical_df = pd.read_parquet(HISTORICAL_PATH)\n",
    "historical_df = historical_df[historical_df[\"species\"].isin(TARGET_SPECIES)].copy()\n",
    "historical_df = add_coarse_bins(historical_df, GRID_DECIMALS)\n",
    "print(f\"  Loaded: {len(historical_df):,} observations\")\n",
    "\n",
    "print(\"Creating train/val split (80/20, stratified by species)...\")\n",
    "train_df, val_df = train_test_split(\n",
    "    historical_df,\n",
    "    test_size=0.2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=historical_df[\"species\"],\n",
    ")\n",
    "\n",
    "print(\"Loading 2025 temporal holdout test data...\")\n",
    "test_df = pd.read_parquet(TEST_PATH)\n",
    "test_df = test_df[test_df[\"species\"].isin(TARGET_SPECIES)].copy()\n",
    "test_df = add_coarse_bins(test_df, GRID_DECIMALS)\n",
    "print(f\"  Loaded: {len(test_df):,} observations\")\n",
    "\n",
    "print(\"\\nSplit summary:\")\n",
    "print(f\"  Train: {len(train_df):,}\")\n",
    "print(f\"  Val:   {len(val_df):,}\")\n",
    "print(f\"  Test:  {len(test_df):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Species distribution per split (counts)\n",
    "dist = (\n",
    "    pd.concat([\n",
    "        train_df.assign(split=\"train\"),\n",
    "        val_df.assign(split=\"val\"),\n",
    "        test_df.assign(split=\"test\"),\n",
    "    ], ignore_index=True)\n",
    "    .groupby([\"split\", \"species\"])\n",
    "    .size()\n",
    "    .unstack(\"split\")\n",
    "    .fillna(0)\n",
    "    .astype(int)\n",
    "    .loc[sorted(TARGET_SPECIES)]\n",
    ")\n",
    "\n",
    "dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 5. Feature engineering\n",
    "Transforms raw columns into model-ready environmental features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 2: FEATURE ENGINEERING (final)\n",
    "# ============================================================================\n",
    "# Notes:\n",
    "# - Assumes your parquet already contains weather + landcover columns\n",
    "#   e.g., temp_c, rhum, wspd_ms, prcp_mm, cloud_cover, swrad, vpd_kpa,\n",
    "#   and worldcover fractions wc{radius}_<class> for radius in {10,50,100,250}.\n",
    "# - If some columns are missing, you can either regenerate the parquet or\n",
    "#   adapt the feature engineering below.\n",
    "\n",
    "def engineer_features(df: pd.DataFrame, verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Create comprehensive environmental features.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure time basics exist (derive if needed)\n",
    "    if \"obs_dt_utc\" in df.columns:\n",
    "        dt_utc = pd.to_datetime(df[\"obs_dt_utc\"], utc=True, errors=\"coerce\")\n",
    "    elif \"observed_at\" in df.columns:\n",
    "        dt_utc = pd.to_datetime(df[\"observed_at\"], utc=True, errors=\"coerce\")\n",
    "    else:\n",
    "        dt_utc = pd.to_datetime(pd.NaT)\n",
    "\n",
    "    if \"hour_local\" not in df.columns:\n",
    "        # derive local hour from UTC timestamp if possible\n",
    "        try:\n",
    "            dt_local = dt_utc.dt.tz_convert(\"Europe/Amsterdam\")\n",
    "            df[\"hour_local\"] = dt_local.dt.hour\n",
    "        except Exception:\n",
    "            df[\"hour_local\"] = np.nan\n",
    "\n",
    "    if \"obs_month\" not in df.columns:\n",
    "        try:\n",
    "            dt_local = dt_utc.dt.tz_convert(\"Europe/Amsterdam\")\n",
    "            df[\"obs_month\"] = dt_local.dt.month\n",
    "        except Exception:\n",
    "            df[\"obs_month\"] = np.nan\n",
    "\n",
    "    if \"doy\" not in df.columns:\n",
    "        try:\n",
    "            dt_local = dt_utc.dt.tz_convert(\"Europe/Amsterdam\")\n",
    "            df[\"doy\"] = dt_local.dt.dayofyear\n",
    "        except Exception:\n",
    "            df[\"doy\"] = np.nan\n",
    "\n",
    "    # TEMPORAL FEATURES\n",
    "    if verbose: print(\"  âœ“ Temporal features...\")\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour_local\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour_local\"] / 24)\n",
    "\n",
    "    week_of_year = dt_utc.dt.isocalendar().week if hasattr(dt_utc, \"dt\") else pd.Series([np.nan]*len(df))\n",
    "    df[\"week_of_year\"] = week_of_year.astype(\"Int64\")\n",
    "    df[\"week_sin\"] = np.sin(2 * np.pi * df[\"week_of_year\"].astype(float) / 52)\n",
    "    df[\"week_cos\"] = np.cos(2 * np.pi * df[\"week_of_year\"].astype(float) / 52)\n",
    "\n",
    "    # Approx. day-length model (same as your thesis code)\n",
    "    day_length = 12 + 6 * np.sin(2 * np.pi * (df[\"doy\"].astype(float) - 80) / 365)\n",
    "    sunrise_hour = 12 - day_length / 2\n",
    "    sunset_hour = 12 + day_length / 2\n",
    "    df[\"hours_since_sunrise\"] = df[\"hour_local\"] - sunrise_hour\n",
    "    df[\"hours_until_sunset\"] = sunset_hour - df[\"hour_local\"]\n",
    "    df[\"is_golden_hour\"] = ((df[\"hours_since_sunrise\"] < 2) | (df[\"hours_until_sunset\"] < 2)).astype(int)\n",
    "\n",
    "    df[\"is_spring\"] = df[\"obs_month\"].isin([3, 4, 5]).astype(int)\n",
    "    df[\"is_summer\"] = df[\"obs_month\"].isin([6, 7, 8]).astype(int)\n",
    "    df[\"is_fall\"]   = df[\"obs_month\"].isin([9, 10]).astype(int)\n",
    "\n",
    "    # WEATHER FEATURES\n",
    "    if verbose: print(\"  âœ“ Weather features...\")\n",
    "    df[\"is_optimal_temp\"] = ((df[\"temp_c\"] >= 15) & (df[\"temp_c\"] <= 28)).astype(int)\n",
    "    df[\"temp_squared\"] = df[\"temp_c\"] ** 2\n",
    "    df[\"is_humid\"] = (df[\"rhum\"] > 70).astype(int)\n",
    "    df[\"is_dry\"] = (df[\"rhum\"] < 40).astype(int)\n",
    "    df[\"is_calm\"] = (df[\"wspd_ms\"] < 3).astype(int)\n",
    "    df[\"is_windy\"] = (df[\"wspd_ms\"] > 7).astype(int)\n",
    "    df[\"has_rain\"] = (df[\"prcp_mm\"] > 0.5).astype(int)\n",
    "    df[\"is_sunny\"] = (df[\"cloud_cover\"] < 30).astype(int)\n",
    "    df[\"is_overcast\"] = (df[\"cloud_cover\"] > 70).astype(int)\n",
    "    df[\"swrad_per_hour\"] = df[\"swrad\"] / np.maximum(day_length, 1)\n",
    "\n",
    "    # HABITAT COMPOSITION\n",
    "    if verbose: print(\"  âœ“ Habitat composition...\")\n",
    "    for radius in [10, 50, 100, 250]:\n",
    "        df[f\"vegetation_total_{radius}\"] = (\n",
    "            df[f\"wc{radius}_tree\"] + df[f\"wc{radius}_shrub\"] + df[f\"wc{radius}_grass\"]\n",
    "        )\n",
    "        df[f\"natural_total_{radius}\"] = (\n",
    "            df[f\"wc{radius}_tree\"] + df[f\"wc{radius}_shrub\"] +\n",
    "            df[f\"wc{radius}_grass\"] + df[f\"wc{radius}_herb_wetland\"]\n",
    "        )\n",
    "        df[f\"impervious_{radius}\"] = df[f\"wc{radius}_builtup\"] + df[f\"wc{radius}_bare\"]\n",
    "\n",
    "    # HABITAT DIVERSITY\n",
    "    if verbose: print(\"  âœ“ Habitat diversity...\")\n",
    "    for radius in [10, 50, 100, 250]:\n",
    "        habitat_cols = [\n",
    "            f\"wc{radius}_tree\", f\"wc{radius}_shrub\", f\"wc{radius}_grass\",\n",
    "            f\"wc{radius}_cropland\", f\"wc{radius}_builtup\", f\"wc{radius}_water\",\n",
    "        ]\n",
    "        habitat_matrix = df[habitat_cols].values + 1e-6\n",
    "        habitat_matrix = habitat_matrix / habitat_matrix.sum(axis=1, keepdims=True)\n",
    "        shannon = -np.sum(habitat_matrix * np.log(habitat_matrix), axis=1)\n",
    "\n",
    "        df[f\"habitat_diversity_{radius}\"] = shannon\n",
    "        df[f\"habitat_richness_{radius}\"] = (df[habitat_cols] > 0.05).sum(axis=1)\n",
    "        df[f\"habitat_dominance_{radius}\"] = df[habitat_cols].max(axis=1)\n",
    "\n",
    "    # CROSS-SCALE GRADIENTS\n",
    "    if verbose: print(\"  âœ“ Cross-scale gradients...\")\n",
    "    df[\"vegetation_gradient_10_50\"] = df[\"vegetation_total_10\"] - df[\"vegetation_total_50\"]\n",
    "    df[\"vegetation_gradient_50_250\"] = df[\"vegetation_total_50\"] - df[\"vegetation_total_250\"]\n",
    "    df[\"urban_gradient_10_50\"] = df[\"wc10_builtup\"] - df[\"wc50_builtup\"]\n",
    "    df[\"urban_gradient_50_250\"] = df[\"wc50_builtup\"] - df[\"wc250_builtup\"]\n",
    "    df[\"water_gradient_10_100\"] = df[\"wc10_water\"] - df[\"wc100_water\"]\n",
    "    df[\"tree_gradient_10_100\"] = df[\"wc10_tree\"] - df[\"wc100_tree\"]\n",
    "\n",
    "    # WEATHER Ã— HABITAT INTERACTIONS\n",
    "    if verbose: print(\"  âœ“ Weather-habitat interactions...\")\n",
    "    df[\"temp_x_vegetation_50\"] = df[\"temp_c\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"temp_x_builtup_50\"] = df[\"temp_c\"] * df[\"wc50_builtup\"]\n",
    "    df[\"temp_x_water_50\"] = df[\"temp_c\"] * df[\"wc50_water\"]\n",
    "    df[\"humidity_x_vegetation_50\"] = df[\"rhum\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"humidity_x_wetland_50\"] = df[\"rhum\"] * df[\"wc50_herb_wetland\"]\n",
    "    df[\"wind_x_vegetation_100\"] = df[\"wspd_ms\"] * df[\"vegetation_total_100\"]\n",
    "    df[\"wind_x_tree_shelter\"] = df[\"wspd_ms\"] * df[\"wc100_tree\"]\n",
    "    df[\"solar_x_vegetation\"] = df[\"swrad\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"solar_x_builtup\"] = df[\"swrad\"] * df[\"wc50_builtup\"]\n",
    "    df[\"vpd_x_vegetation\"] = df[\"vpd_kpa\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"vpd_x_water_proximity\"] = df[\"vpd_kpa\"] * (1 - df[\"wc50_water\"])\n",
    "\n",
    "    # URBAN CONTEXT\n",
    "    if verbose: print(\"  âœ“ Urban context...\")\n",
    "    df[\"urban_heat_index\"] = df[\"wc50_builtup\"] * 2 + df[\"wc250_builtup\"] - 0.5 * df[\"vegetation_total_50\"]\n",
    "    df[\"floral_resources\"] = (\n",
    "        df[\"wc10_grass\"] * 0.5 + df[\"wc50_grass\"] * 1.0 +\n",
    "        df[\"wc50_shrub\"] * 1.5 + df[\"wc50_cropland\"] * 0.8\n",
    "    )\n",
    "    df[\"cavity_nesting_habitat\"] = df[\"wc50_tree\"] + df[\"wc50_builtup\"] * 0.2\n",
    "    df[\"ground_nesting_habitat\"] = df[\"wc10_grass\"] + df[\"wc10_bare\"] * 0.5\n",
    "    df[\"habitat_edges_50\"] = df[\"habitat_richness_50\"] * df[\"habitat_diversity_50\"]\n",
    "\n",
    "    # TEMPORAL Ã— HABITAT INTERACTIONS\n",
    "    if verbose: print(\"  âœ“ Temporal-habitat interactions...\")\n",
    "    df[\"spring_x_vegetation\"] = df[\"is_spring\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"summer_x_water\"] = df[\"is_summer\"] * df[\"wc50_water\"]\n",
    "    df[\"morning_x_flowers\"] = (df[\"hour_local\"] < 12).astype(int) * df[\"floral_resources\"]\n",
    "    df[\"afternoon_x_flowers\"] = (df[\"hour_local\"] >= 12).astype(int) * df[\"floral_resources\"]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"âœ… Feature engineering complete! ({len(df.columns)} columns)\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Engineering features...\")\n",
    "train_df = engineer_features(train_df, verbose=True)\n",
    "val_df   = engineer_features(val_df, verbose=False)\n",
    "test_df  = engineer_features(test_df, verbose=False)\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Feature selection and matrices\n",
    "Drops identifiers and potential leakage (coordinates), removes zero-importance features, and builds `X_*` / `y_*`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 3: PREPARE FEATURE MATRICES\n",
    "# ============================================================================\n",
    "\n",
    "EXCLUDE_COLS = [\n",
    "    'observation_id', 'gbifID', 'taxon_name', 'species', 'speciesKey',\n",
    "    'observed_at', 'obs_dt_local_naive', 'obs_dt_utc', 'obs_dt_hour_utc', 'day_utc',\n",
    "    'dataGeneralizations', 'recordedBy', 'individualCount', 'sex', 'lifeStage',\n",
    "    'level0Name', 'level1Name', 'level2Name', 'mediaURL', 'mediaLicense',\n",
    "    'mediaURL_is_image', 'source_ref', 'occurrenceLicense',\n",
    "    'precise_latitude', 'precise_longitude', 'extraction_success', 'extraction_message',\n",
    "    'coord_unc_m', 'eff_unc_m', 'is_generalized',\n",
    "    'week_of_year',\n",
    "    # EXCLUDE COORDINATES (no spatial leakage)\n",
    "    'latitude', 'longitude', \n",
    "    'final_latitude', 'final_longitude', \n",
    "    'obs_day', 'obs_hour', 'obs_min', 'time_imputed', 'gps_accuracy_m',\n",
    "    'obs_year','obs_month', \n",
    "     #\"is_spring\", \"is_fall\", \"is_summer\", \n",
    "     #\"doy\", \n",
    "     #'hour_sin'\n",
    "     #\"lat_bin\", \"lon_bin\",\n",
    "     #'final_accuracy_m', \n",
    "     #'temp_squared'\n",
    "]\n",
    "\n",
    "# âœ… EXCLUDE ZERO-IMPORTANCE FEATURES\n",
    "ZERO_IMPORTANCE_FEATURES = [\n",
    "    # Snow/ice features (not relevant in Netherlands urban environment)\n",
    "    'wc10_snowice', 'wc50_snowice', 'wc100_snowice', 'wc250_snowice',\n",
    "    'snow_mm',\n",
    "    \n",
    "    # Shrub features (minimal urban coverage)\n",
    "    'wc10_shrub', 'wc50_shrub', 'wc100_shrub', 'wc250_shrub',\n",
    "    \n",
    "    # Moss/lichen features (not relevant for these species)\n",
    "    'wc10_moss_lichen', 'wc50_moss_lichen', 'wc100_moss_lichen', 'wc250_moss_lichen',\n",
    "    \n",
    "    # Mangrove features (not present in Netherlands)\n",
    "    'wc10_mangroves', 'wc50_mangroves', 'wc100_mangroves', 'wc250_mangroves',\n",
    "    \n",
    "    # Other zero-importance features\n",
    "    'wc10_cropland',  # Urban environment\n",
    "    'wc10_bare',      # Minimal bare ground at 10m scale\n",
    "]\n",
    "\n",
    "\n",
    "def select_feature_columns(df: pd.DataFrame) -> list[str]:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        if c in EXCLUDE_COLS or c in ZERO_IMPORTANCE_FEATURES:\n",
    "            continue\n",
    "        # keep only numeric columns\n",
    "        if not pd.api.types.is_numeric_dtype(df[c]):\n",
    "            continue\n",
    "        # skip all-NA columns\n",
    "        if df[c].notna().sum() == 0:\n",
    "            continue\n",
    "        cols.append(c)\n",
    "    return cols\n",
    "\n",
    "feature_cols = select_feature_columns(train_df)\n",
    "\n",
    "print(f\"Selected {len(feature_cols)} numeric features\")\n",
    "\n",
    "X_train = train_df[feature_cols].fillna(0).values\n",
    "X_val   = val_df[feature_cols].fillna(0).values\n",
    "X_test  = test_df[feature_cols].fillna(0).values\n",
    "\n",
    "species_to_idx = {sp: i for i, sp in enumerate(sorted(TARGET_SPECIES))}\n",
    "idx_to_species = {i: sp for sp, i in species_to_idx.items()}\n",
    "\n",
    "y_train = train_df[\"species\"].map(species_to_idx).values\n",
    "y_val   = val_df[\"species\"].map(species_to_idx).values\n",
    "y_test  = test_df[\"species\"].map(species_to_idx).values\n",
    "\n",
    "sample_weights = compute_sample_weight(\"balanced\", y=y_train)\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"  X_train:\", X_train.shape)\n",
    "print(\"  X_val:  \", X_val.shape)\n",
    "print(\"  X_test: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter comparison\n",
    "Quick comparison across a small set of configs. Early stopping uses validation performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: QUICK HYPERPARAMETER COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "def top_k_accuracy(y_true: np.ndarray, y_proba: np.ndarray, k: int) -> float:\n",
    "    top_k = np.argsort(y_proba, axis=1)[:, -k:]\n",
    "    return float(np.mean([y_true[i] in top_k[i] for i in range(len(y_true))]))\n",
    "\n",
    "base_params = {\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"num_class\": len(TARGET_SPECIES),\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.7,\n",
    "    \"colsample_bylevel\": 0.7,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"gamma\": 0.1,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"reg_lambda\": 1.0,\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"n_jobs\": -1,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "    \"eval_metric\": [\"mlogloss\", \"merror\"],\n",
    "}\n",
    "\n",
    "configs = [\n",
    "    {\"name\": \"Config A (More Regularization)\", \"max_depth\": 5, \"learning_rate\": 0.05, \"n_estimators\": 500},\n",
    "    {\"name\": \"Config B (Baseline)\",           \"max_depth\": 6, \"learning_rate\": 0.05, \"n_estimators\": 500},\n",
    "    {\"name\": \"Config C (More Capacity)\",      \"max_depth\": 7, \"learning_rate\": 0.03, \"n_estimators\": 500},\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for cfg in configs:\n",
    "    params = dict(base_params)\n",
    "    params.update({k: v for k, v in cfg.items() if k != \"name\"})\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        sample_weight=sample_weights,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_proba_val = model.predict_proba(X_val)\n",
    "\n",
    "    results.append({\n",
    "        \"name\": cfg[\"name\"],\n",
    "        \"config\": cfg,\n",
    "        \"model\": model,\n",
    "        \"val_top1\": accuracy_score(y_val, y_pred_val),\n",
    "        \"val_top3\": top_k_accuracy(y_val, y_proba_val, 3),\n",
    "        \"val_top5\": top_k_accuracy(y_val, y_proba_val, 5),\n",
    "        \"val_logloss\": log_loss(y_val, y_proba_val),\n",
    "        \"best_iteration\": getattr(model, \"best_iteration\", None),\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame([{k: v for k, v in r.items() if k not in [\"model\",\"config\"]} for r in results])\n",
    "df_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 8. Select best configuration\n",
    "Selection criterion: maximise validation Top-3 accuracy, then minimise validation log loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick best config: highest Top-3, then lowest logloss\n",
    "best = max(results, key=lambda r: (r[\"val_top3\"], -r[\"val_logloss\"]))\n",
    "model = best[\"model\"]\n",
    "print(\"Best:\", best[\"name\"])\n",
    "print(best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 9. Final evaluation\n",
    "Reports Top-k accuracy and log loss across train/val/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: FINAL EVALUATION (Train / Val / Test)\n",
    "# ============================================================================\n",
    "\n",
    "y_pred_train = model.predict(X_train)\n",
    "y_pred_val   = model.predict(X_val)\n",
    "y_pred_test  = model.predict(X_test)\n",
    "\n",
    "y_proba_train = model.predict_proba(X_train)\n",
    "y_proba_val   = model.predict_proba(X_val)\n",
    "y_proba_test  = model.predict_proba(X_test)\n",
    "\n",
    "print(f\"{'Split':<10} {'Top-1':>8} {'Top-3':>8} {'Top-5':>8} {'Log Loss':>10}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"{'Train':<10} {accuracy_score(y_train, y_pred_train):>8.1%} {top_k_accuracy(y_train, y_proba_train, 3):>8.1%} \"\n",
    "      f\"{top_k_accuracy(y_train, y_proba_train, 5):>8.1%} {log_loss(y_train, y_proba_train):>10.4f}\")\n",
    "print(f\"{'Val':<10}   {accuracy_score(y_val, y_pred_val):>8.1%}   {top_k_accuracy(y_val, y_proba_val, 3):>8.1%} \"\n",
    "      f\"{top_k_accuracy(y_val, y_proba_val, 5):>8.1%}   {log_loss(y_val, y_proba_val):>10.4f}\")\n",
    "print(f\"{'Test':<10}  {accuracy_score(y_test, y_pred_test):>8.1%}  {top_k_accuracy(y_test, y_proba_test, 3):>8.1%} \"\n",
    "      f\"{top_k_accuracy(y_test, y_proba_test, 5):>8.1%}  {log_loss(y_test, y_proba_test):>10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 10. Detailed test report\n",
    "Per-class precision/recall/F1 on the 2025 temporal holdout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST SET CLASSIFICATION REPORT (2025 Temporal Holdout)\")\n",
    "print(\"=\"*80)\n",
    "print(classification_report(\n",
    "    y_test, y_pred_test,\n",
    "    target_names=[idx_to_species[i] for i in range(len(TARGET_SPECIES))],\n",
    "    digits=3,\n",
    "    zero_division=0,\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 11. Calibration quality\n",
    "Multi-class (normalised) Brier score and per-species Brier on test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 5B: BRIER SCORE (multi-class)\n",
    "# ============================================================================\n",
    "\n",
    "def multiclass_brier_score(y_true: np.ndarray, y_proba: np.ndarray) -> float:\n",
    "    n_samples, n_classes = y_proba.shape\n",
    "    brier = 0.0\n",
    "    for i in range(n_samples):\n",
    "        for k in range(n_classes):\n",
    "            y_true_k = 1 if y_true[i] == k else 0\n",
    "            brier += (y_proba[i, k] - y_true_k) ** 2\n",
    "    return brier / (n_samples * n_classes)\n",
    "\n",
    "brier_train = multiclass_brier_score(y_train, y_proba_train)\n",
    "brier_val   = multiclass_brier_score(y_val, y_proba_val)\n",
    "brier_test  = multiclass_brier_score(y_test, y_proba_test)\n",
    "\n",
    "print(\"Brier (multi-class):\")\n",
    "print(\"  Train:\", round(brier_train, 4))\n",
    "print(\"  Val:  \", round(brier_val, 4))\n",
    "print(\"  Test: \", round(brier_test, 4))\n",
    "\n",
    "# Per-species Brier on test\n",
    "species_brier_scores = []\n",
    "for k, sp in enumerate(sorted(TARGET_SPECIES)):\n",
    "    y_true_k = (y_test == k).astype(int)\n",
    "    y_pred_k = y_proba_test[:, k]\n",
    "    b = float(np.mean((y_pred_k - y_true_k) ** 2))\n",
    "    species_brier_scores.append({\"species\": sp, \"brier\": b})\n",
    "\n",
    "pd.DataFrame(species_brier_scores).sort_values(\"brier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 12. Save artefacts\n",
    "Writes the model, feature list, species mapping, test probabilities, and summary metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: SAVE MODEL + METADATA + PREDICTIONS\n",
    "# ============================================================================\n",
    "\n",
    "# Model\n",
    "model_path = MODEL_DIR / \"xgboost_hsi_model_FINAL_no_vespula.json\"\n",
    "model.save_model(str(model_path))\n",
    "print(\"Saved model:\", model_path)\n",
    "\n",
    "# Feature names + species mapping (needed for inference/fusion)\n",
    "(pd.DataFrame({\"feature\": feature_cols})\n",
    "   .to_csv(MODEL_DIR / \"feature_names_FINAL_no_vespula.csv\", index=False))\n",
    "\n",
    "(pd.DataFrame([{\"species\": sp, \"idx\": idx} for sp, idx in species_to_idx.items()])\n",
    "   .to_csv(MODEL_DIR / \"species_mapping_FINAL_no_vespula.csv\", index=False))\n",
    "\n",
    "print(\"Saved feature names + mapping in:\", MODEL_DIR)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Test predictions parquet (for fusion)\n",
    "# ---------------------------------------------------------------------------\n",
    "if \"observation_id\" in test_df.columns:\n",
    "    test_preds_df = test_df[[\"observation_id\", \"species\"]].copy()\n",
    "else:\n",
    "    test_preds_df = test_df[[\"species\"]].copy()\n",
    "\n",
    "for i, sp in enumerate(sorted(TARGET_SPECIES)):\n",
    "    col = f\"prob_{sp.replace(' ', '_')}\"\n",
    "    test_preds_df[col] = y_proba_test[:, i]\n",
    "\n",
    "test_preds_df[\"predicted_species\"] = [idx_to_species[i] for i in y_pred_test]\n",
    "\n",
    "test_pred_path = OUT_DIR / \"preds\" / \"test_predictions_2025_FINAL_no_vespula.parquet\"\n",
    "test_preds_df.to_parquet(test_pred_path, index=False)\n",
    "print(\"Saved test predictions:\", test_pred_path)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Metrics\n",
    "# ---------------------------------------------------------------------------\n",
    "summary_metrics = pd.DataFrame({\n",
    "    \"metric\": [\"top_1_accuracy\", \"top_3_accuracy\", \"top_5_accuracy\", \"log_loss\", \"brier_score\"],\n",
    "    \"test_value\": [\n",
    "        accuracy_score(y_test, y_pred_test),\n",
    "        top_k_accuracy(y_test, y_proba_test, 3),\n",
    "        top_k_accuracy(y_test, y_proba_test, 5),\n",
    "        log_loss(y_test, y_proba_test),\n",
    "        brier_test,\n",
    "    ],\n",
    "})\n",
    "summary_metrics.to_csv(OUT_DIR / \"metrics\" / \"summary_metrics_FINAL.csv\", index=False)\n",
    "\n",
    "pd.DataFrame(species_brier_scores).to_csv(OUT_DIR / \"metrics\" / \"brier_scores_per_species.csv\", index=False)\n",
    "\n",
    "print(\"Saved metrics in:\", OUT_DIR / \"metrics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% 6) SAVE PREDICTIONS WITH PROBABILITIES + CONTEXT\n",
    "# Species mapping (order matters!)\n",
    "species_map = pd.read_csv(SPECIES_MAP_PATH).sort_values(\"idx\")\n",
    "species_names = species_map[\"species\"].tolist()\n",
    "def save_predictions_with_context(df, y_true, y_proba, species_names, split_name, out_dir=None):\n",
    "    \"\"\"Save predictions with context for analysis (robust column selection).\"\"\"\n",
    "    print(f\"\\nðŸ’¾ Saving {split_name} predictions with context...\")\n",
    "\n",
    "    if out_dir is None:\n",
    "        out_dir = OUT_DIR / \"preds\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Pick a sensible set of context columns if available\n",
    "    preferred_cols = [\n",
    "        \"observation_id\",\n",
    "        \"species\",\n",
    "        \"final_latitude\", \"final_longitude\",\n",
    "        \"latitude\", \"longitude\",\n",
    "        \"observed_at\", \"obs_dt_utc\",\n",
    "        \"temp_c\", \"hour_local\", \"doy\",\n",
    "        \"rhum\", \"wspd_ms\", \"prcp_mm\", \"cloud_cover\", \"swrad\", \"vpd_kpa\",\n",
    "        \"lat_bin\", \"lon_bin\",\n",
    "    ]\n",
    "    context_cols = [c for c in preferred_cols if c in df.columns]\n",
    "    pred_df = df[context_cols].copy()\n",
    "\n",
    "    # Truth / prediction\n",
    "    pred_df[\"true_species\"] = df[\"species\"].astype(str).values if \"species\" in df.columns else y_true\n",
    "    pred_df[\"true_species_idx\"] = y_true\n",
    "    pred_df[\"predicted_species_idx\"] = y_proba.argmax(axis=1)\n",
    "    pred_df[\"predicted_species\"] = [species_names[i] for i in pred_df[\"predicted_species_idx\"]]\n",
    "    pred_df[\"max_probability\"] = y_proba.max(axis=1)\n",
    "    pred_df[\"correct\"] = (y_true == y_proba.argmax(axis=1)).astype(int)\n",
    "\n",
    "    # Top-3 predictions\n",
    "    top3_idx = np.argsort(y_proba, axis=1)[:, -3:][:, ::-1]\n",
    "    for k in range(3):\n",
    "        pred_df[f\"top{k+1}_species\"] = [species_names[i] for i in top3_idx[:, k]]\n",
    "        pred_df[f\"top{k+1}_probability\"] = y_proba[np.arange(len(y_proba)), top3_idx[:, k]]\n",
    "\n",
    "    # Per-species probabilities\n",
    "    for i, sp in enumerate(species_names):\n",
    "        safe_name = sp.replace(\" \", \"_\").replace(\".\", \"\")\n",
    "        pred_df[f\"prob_{safe_name}\"] = y_proba[:, i]\n",
    "\n",
    "    out_path = out_dir / f\"{split_name}_predictions_with_hsi.parquet\"\n",
    "    pred_df.to_parquet(out_path, index=False)\n",
    "    print(f\"âœ… Saved {len(pred_df):,} predictions to: {out_path}\")\n",
    "\n",
    "    # sample CSV (handy for manual inspection)\n",
    "    sample = pred_df.sample(min(500, len(pred_df)), random_state=42)\n",
    "    sample_path = out_dir / f\"{split_name}_predictions_sample.csv\"\n",
    "    sample.to_csv(sample_path, index=False)\n",
    "    print(f\"âœ… Saved sample CSV to: {sample_path}\")\n",
    "\n",
    "    return pred_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = save_predictions_with_context(test_df, y_test, y_proba_test, species_names, \"test\")\n",
    "_ = save_predictions_with_context(val_df,  y_val,  y_proba_val,  species_names, \"val\")\n",
    "_ = save_predictions_with_context(train_df, y_train, y_proba_train, species_names, \"train\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- For probability calibration (isotonic / sigmoid with spatial CV) and model diagnostics (feature importance, confusion matrix, reliability diagrams), keep these in a separate analysis notebook to keep this training notebook focused and lightweight.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis-py312)",
   "language": "python",
   "name": "thesis-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
