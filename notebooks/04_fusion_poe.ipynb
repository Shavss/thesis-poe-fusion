{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fusion evaluation: HSI √ó ResNet-18 (10 runs)\n",
    "\n",
    "This notebook evaluates **late fusion** between:\n",
    "\n",
    "- **Vision expert:** 10 independently trained ResNet-18 models (same architecture, different seeds)\n",
    "- **Context expert:** XGBoost Habitat Suitability Index (HSI)\n",
    "- **Fusion rule:** Product of Experts (PoE):  \n",
    "$$\n",
    "p(y \\mid x, c) \\propto p_{\\text{vis}}(y \\mid x)\\, p_{\\text{ctx}}(y \\mid c)\n",
    "$$\n",
    "\n",
    "What you get:\n",
    "- Accuracy (Top-1 / Top-3 / Top-5) for **ResNet**, **HSI**, **Fused**\n",
    "- Calibration metrics for **ResNet** and **Fused**: ECE (top-label), multi-class Brier, log-loss\n",
    "- McNemar test (paired) per run: ResNet vs Fused\n",
    "- Mean confusion matrices across all 10 runs\n",
    "- Clean, repo-relative paths (works even if you run from `notebooks/`)\n",
    "\n",
    "**Expected repo layout (yours):**\n",
    "- Models: `models/vision/*.pth`\n",
    "- Temperatures: `models/vision/temperatures/temperature_<run_tag>.npy`\n",
    "- HSI model + mapping: `models/context/`\n",
    "- Outputs: `outputs/fusion_poe/{figures,metrics,preds}`\n",
    "- Data: `data/amsterdam/images_no_vespula/test2` and metadata parquet under `data/amsterdam/val/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, platform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"MPS available:\", getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Optional: reproducibility baseline\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1) Configuration (repo-relative paths)\n",
    "\n",
    "This cell finds the repo root via `pyproject.toml` and defines all paths relative to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch  # ‚úÖ needed for DEVICE\n",
    "\n",
    "# Single source of truth for paths\n",
    "from digital_naturalist.paths import load_paths\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Load paths (repo-relative, portable)\n",
    "# -------------------------------------------------------------------\n",
    "P = load_paths(\"configs/paths.yaml\")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Outputs (fusion)\n",
    "# -------------------------------------------------------------------\n",
    "OUT_DIR = P[\"OUT_FUSION_POE\"]\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "MET_DIR = OUT_DIR / \"metrics\"\n",
    "PRD_DIR = OUT_DIR / \"preds\"\n",
    "REPO_ROOT = P[\"REPO_ROOT\"]\n",
    "\n",
    "# Shared logs directory (repo-wide)\n",
    "LOG_DIR = P[\"OUTPUTS_ROOT\"] / \"logs\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Models\n",
    "# -------------------------------------------------------------------\n",
    "VISION_MODEL_DIR = P[\"VISION_MODEL_DIR\"]\n",
    "VISION_TEMPS_DIR = P[\"VISION_TEMPS_DIR\"]\n",
    "CONTEXT_MODEL_DIR = P[\"CONTEXT_MODEL_DIR\"]\n",
    "\n",
    "# Context model artifacts (filenames live inside models/context)\n",
    "HSI_MODEL_PATH = CONTEXT_MODEL_DIR / \"xgboost_hsi_model_FINAL_no_vespula.json\"\n",
    "FEATURES_PATH = CONTEXT_MODEL_DIR / \"feature_names_FINAL_no_vespula.csv\"\n",
    "SPECIES_MAP_PATH = CONTEXT_MODEL_DIR / \"species_mapping_FINAL_no_vespula.csv\"\n",
    "\n",
    "# Backwards-compatible aliases (old cells may use these names)\n",
    "FEATURE_NAMES_PATH = FEATURES_PATH\n",
    "SPECIES_MAPPING_PATH = SPECIES_MAP_PATH  # ‚úÖ some cells use this name\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Choose which image split to fuse\n",
    "# Default: temporal holdout \"test2\" if present, else fallback to \"test\"\n",
    "# -------------------------------------------------------------------\n",
    "TEST_SPLIT_NAME = \"test2\"  # ‚úÖ used in logs/labels in many notebooks\n",
    "\n",
    "IMAGE_SPLIT_DIR = P.get(\"IMAGE_TEST2_DIR\", None)\n",
    "if IMAGE_SPLIT_DIR is None:\n",
    "    IMAGE_SPLIT_DIR = P[\"IMAGE_TEST_DIR\"]\n",
    "    TEST_SPLIT_NAME = \"test\"\n",
    "\n",
    "# Backwards-compatible alias used in older pairing code\n",
    "IMAGE_ROOT = IMAGE_SPLIT_DIR  # ‚úÖ your old code uses IMAGE_ROOT\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Metadata pool for synthetic pairing (GBIF val parquet)\n",
    "# (Used to sample context rows per species to match images 1:1)\n",
    "# -------------------------------------------------------------------\n",
    "GBIF_VAL_DIR = P[\"GBIF_VAL_DIR\"]\n",
    "GBIF_VAL_PARQUET = GBIF_VAL_DIR / \"observations_filtered_50m_accuracy.parquet\"\n",
    "\n",
    "# Backwards-compatible alias \n",
    "METADATA_PATH = GBIF_VAL_PARQUET  \n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Run / fusion settings (often assumed later)\n",
    "# -------------------------------------------------------------------\n",
    "NUM_CLASSES = 8\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "GRID_DECIMALS = 2  # MUST match HSI training\n",
    "SCALE = 10 ** GRID_DECIMALS\n",
    "\n",
    "SEED_PAIRING = 42  # synthetic image‚Üîmetadata pairing seed\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"mps\" if getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available()\n",
    "    else \"cuda\" if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Ensure output dirs exist\n",
    "# -------------------------------------------------------------------\n",
    "for d in (FIG_DIR, MET_DIR, PRD_DIR, LOG_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Sanity checks (fail early with clear error)\n",
    "# -------------------------------------------------------------------\n",
    "assert IMAGE_SPLIT_DIR.exists(), f\"Missing image split dir: {IMAGE_SPLIT_DIR}\"\n",
    "assert VISION_MODEL_DIR.exists(), f\"Missing vision model dir: {VISION_MODEL_DIR}\"\n",
    "assert VISION_TEMPS_DIR.exists(), f\"Missing vision temps dir: {VISION_TEMPS_DIR}\"\n",
    "assert CONTEXT_MODEL_DIR.exists(), f\"Missing context model dir: {CONTEXT_MODEL_DIR}\"\n",
    "\n",
    "# These three may be absent if you're preparing a ‚Äúno-models‚Äù repo,\n",
    "# but for running fusion locally they must exist:\n",
    "assert HSI_MODEL_PATH.exists(), f\"Missing HSI model: {HSI_MODEL_PATH}\"\n",
    "assert FEATURES_PATH.exists(), f\"Missing feature names CSV: {FEATURES_PATH}\"\n",
    "assert SPECIES_MAP_PATH.exists(), f\"Missing species mapping CSV: {SPECIES_MAP_PATH}\"\n",
    "assert GBIF_VAL_PARQUET.exists(), f\"Missing GBIF val parquet: {GBIF_VAL_PARQUET}\"\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Printout\n",
    "# -------------------------------------------------------------------\n",
    "print(\"\\n=== Fusion PoE ‚Äî resolved paths ===\")\n",
    "print(\"Config:              configs/paths.yaml\")\n",
    "print(\"Device:              \", DEVICE)\n",
    "print(\"Test split name:     \", TEST_SPLIT_NAME)\n",
    "print(\"Image split dir:     \", IMAGE_SPLIT_DIR)\n",
    "print(\"GBIF val parquet:    \", GBIF_VAL_PARQUET)\n",
    "print(\"Vision models dir:   \", VISION_MODEL_DIR)\n",
    "print(\"Vision temps dir:    \", VISION_TEMPS_DIR)\n",
    "print(\"Context model dir:   \", CONTEXT_MODEL_DIR)\n",
    "print(\"HSI model:           \", HSI_MODEL_PATH)\n",
    "print(\"Feature names:       \", FEATURES_PATH)\n",
    "print(\"Species mapping:     \", SPECIES_MAP_PATH)\n",
    "print(\"Outputs/figures:     \", FIG_DIR)\n",
    "print(\"Outputs/metrics:     \", MET_DIR)\n",
    "print(\"Outputs/preds:       \", PRD_DIR)\n",
    "print(\"Logs:                \", LOG_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2) Load wild test images and metadata pool\n",
    "\n",
    "We have:\n",
    "- **Images:** labeled wild camera images (Aarhus rooftop camera) stored in an `ImageFolder` structure.\n",
    "- **Metadata pool:** Amsterdam GBIF observations with weather + habitat features.\n",
    "\n",
    "**Important:** The pairing of each test image to a metadata row is *synthetic* (matched only by species) to test whether ecological priors can rescue visually ambiguous cases under cross-sensor domain shift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# -------------------------\n",
    "# Synthetic pairing set-up\n",
    "# -------------------------\n",
    "SEED_PAIRING = 42  # controls deterministic image‚Üîcontext pairing\n",
    "rng = np.random.default_rng(SEED_PAIRING)\n",
    "\n",
    "# Species list (must match BOTH: ImageFolder class folders AND GBIF parquet 'species' names)\n",
    "WILD_SPECIES = [\n",
    "    \"Apis mellifera\", \"Eristalis tenax\",\n",
    "    \"Bombus terrestris\", \"Coccinella septempunctata\",\n",
    "    \"Bombus lapidarius\", \"Episyrphus balteatus\",\n",
    "    \"Aglais urticae\", \"Eupeodes corollae\",\n",
    "]\n",
    "\n",
    "# Load metadata pool (context rows)\n",
    "metadata = pd.read_parquet(GBIF_VAL_PARQUET)\n",
    "metadata = metadata[metadata[\"species\"].isin(WILD_SPECIES)].copy()\n",
    "print(\"Metadata rows:\", len(metadata), \"| parquet:\", GBIF_VAL_PARQUET)\n",
    "\n",
    "# Load images ONLY from the chosen split folder (e.g., .../images_no_vespula/test2/<class>/...)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "test_dataset = datasets.ImageFolder(str(IMAGE_SPLIT_DIR), transform=test_transform)\n",
    "print(\"Images in split:\", len(test_dataset), \"| split:\", IMAGE_SPLIT_DIR)\n",
    "print(\"ImageFolder classes:\", [c.replace(\"_\", \" \") for c in test_dataset.classes])\n",
    "\n",
    "# Build a dataframe of image paths + species (from ImageFolder labels)\n",
    "image_df = pd.DataFrame(\n",
    "    [{\n",
    "        \"image_path\": p,\n",
    "        \"species\": test_dataset.classes[y].replace(\"_\", \" \"),\n",
    "        \"resnet_label_idx\": int(y),  # index in ImageFolder class order\n",
    "    } for p, y in test_dataset.samples]\n",
    ")\n",
    "\n",
    "# Synthetic 1:1 pairing by species (sample metadata WITH replacement if needed)\n",
    "matched_rows = []\n",
    "for sp in WILD_SPECIES:\n",
    "    imgs = image_df[image_df[\"species\"] == sp]\n",
    "    metas = metadata[metadata[\"species\"] == sp]\n",
    "    if len(imgs) == 0 or len(metas) == 0:\n",
    "        print(f\"‚ö†Ô∏è  Skipping species (missing imgs or metas): {sp} | imgs={len(imgs)} metas={len(metas)}\")\n",
    "        continue\n",
    "\n",
    "    replace = len(metas) < len(imgs)\n",
    "    chosen = rng.choice(metas.index.to_numpy(), size=len(imgs), replace=replace)\n",
    "\n",
    "    for img_path, meta_idx in zip(imgs[\"image_path\"].tolist(), chosen.tolist()):\n",
    "        matched_rows.append({\n",
    "            \"image_path\": img_path,\n",
    "            \"species\": sp,\n",
    "            \"metadata_index\": int(meta_idx),\n",
    "        })\n",
    "\n",
    "matched_df = pd.DataFrame(matched_rows).reset_index(drop=True)\n",
    "print(\"Matched pairs:\", len(matched_df))\n",
    "\n",
    "# Materialize metadata rows in the same order as matched_df\n",
    "metadata_for_hsi = metadata.loc[matched_df[\"metadata_index\"].to_numpy()].reset_index(drop=True)\n",
    "assert len(metadata_for_hsi) == len(matched_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3) Load HSI model and compute context probabilities\n",
    "\n",
    "The `engineer_hsi_features` function **must match** the feature engineering used in your HSI training script. This is the same logic you used before (kept intact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load context model\n",
    "hsi_model = xgb.XGBClassifier()\n",
    "hsi_model.load_model(str(HSI_MODEL_PATH))\n",
    "\n",
    "feature_names = pd.read_csv(FEATURE_NAMES_PATH)\n",
    "# accept either column name 'feature' or first column\n",
    "if \"feature\" in feature_names.columns:\n",
    "    feature_list = feature_names[\"feature\"].tolist()\n",
    "else:\n",
    "    feature_list = feature_names.iloc[:, 0].tolist()\n",
    "\n",
    "species_mapping = pd.read_csv(SPECIES_MAPPING_PATH)\n",
    "species_to_idx = dict(zip(species_mapping[\"species\"], species_mapping[\"idx\"]))\n",
    "idx_to_species = dict(zip(species_mapping[\"idx\"], species_mapping[\"species\"]))\n",
    "\n",
    "print(\"HSI features:\", len(feature_list))\n",
    "print(\"Species mapping:\", len(species_to_idx))\n",
    "\n",
    "# -------- Feature engineering (must match training) --------\n",
    "def engineer_hsi_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # Spatial bins (lat_bin / lon_bin)\n",
    "    lat = pd.to_numeric(df[\"final_latitude\"], errors=\"coerce\")\n",
    "    lon = pd.to_numeric(df[\"final_longitude\"], errors=\"coerce\")\n",
    "    df[\"lat_bin\"] = np.round(lat * SCALE).astype(\"Int64\")\n",
    "    df[\"lon_bin\"] = np.round(lon * SCALE).astype(\"Int64\")\n",
    "\n",
    "    # Temporal features\n",
    "    df[\"hour_sin\"] = np.sin(2 * np.pi * df[\"hour_local\"] / 24)\n",
    "    df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"hour_local\"] / 24)\n",
    "    df[\"week_of_year\"] = pd.to_datetime(df[\"obs_dt_utc\"], unit=\"ms\").dt.isocalendar().week\n",
    "    df[\"week_sin\"] = np.sin(2 * np.pi * df[\"week_of_year\"] / 52)\n",
    "    df[\"week_cos\"] = np.cos(2 * np.pi * df[\"week_of_year\"] / 52)\n",
    "\n",
    "    day_length = 12 + 6 * np.sin(2 * np.pi * (df[\"doy\"] - 80) / 365)\n",
    "    sunrise_hour = 12 - day_length / 2\n",
    "    sunset_hour = 12 + day_length / 2\n",
    "    df[\"hours_since_sunrise\"] = df[\"hour_local\"] - sunrise_hour\n",
    "    df[\"hours_until_sunset\"] = sunset_hour - df[\"hour_local\"]\n",
    "    df[\"is_golden_hour\"] = ((df[\"hours_since_sunrise\"] < 2) | (df[\"hours_until_sunset\"] < 2)).astype(int)\n",
    "\n",
    "    df[\"is_spring\"] = df[\"obs_month\"].isin([3, 4, 5]).astype(int)\n",
    "    df[\"is_summer\"] = df[\"obs_month\"].isin([6, 7, 8]).astype(int)\n",
    "    df[\"is_fall\"] = df[\"obs_month\"].isin([9, 10]).astype(int)\n",
    "\n",
    "    # Weather features\n",
    "    df[\"is_optimal_temp\"] = ((df[\"temp_c\"] >= 15) & (df[\"temp_c\"] <= 28)).astype(int)\n",
    "    df[\"temp_squared\"] = df[\"temp_c\"] ** 2\n",
    "    df[\"is_humid\"] = (df[\"rhum\"] > 70).astype(int)\n",
    "    df[\"is_dry\"] = (df[\"rhum\"] < 40).astype(int)\n",
    "    df[\"is_calm\"] = (df[\"wspd_ms\"] < 3).astype(int)\n",
    "    df[\"is_windy\"] = (df[\"wspd_ms\"] > 7).astype(int)\n",
    "    df[\"has_rain\"] = (df[\"prcp_mm\"] > 0.5).astype(int)\n",
    "    df[\"is_sunny\"] = (df[\"cloud_cover\"] < 30).astype(int)\n",
    "    df[\"is_overcast\"] = (df[\"cloud_cover\"] > 70).astype(int)\n",
    "    df[\"swrad_per_hour\"] = df[\"swrad\"] / np.maximum(day_length, 1)\n",
    "\n",
    "    # Habitat composition\n",
    "    for radius in [10, 50, 100, 250]:\n",
    "        df[f\"vegetation_total_{radius}\"] = (\n",
    "            df[f\"wc{radius}_tree\"] + df[f\"wc{radius}_shrub\"] + df[f\"wc{radius}_grass\"]\n",
    "        )\n",
    "        df[f\"natural_total_{radius}\"] = (\n",
    "            df[f\"wc{radius}_tree\"] + df[f\"wc{radius}_shrub\"] +\n",
    "            df[f\"wc{radius}_grass\"] + df[f\"wc{radius}_herb_wetland\"]\n",
    "        )\n",
    "        df[f\"impervious_{radius}\"] = df[f\"wc{radius}_builtup\"] + df[f\"wc{radius}_bare\"]\n",
    "\n",
    "    # Habitat diversity\n",
    "    for radius in [10, 50, 100, 250]:\n",
    "        habitat_cols = [\n",
    "            f\"wc{radius}_tree\", f\"wc{radius}_shrub\", f\"wc{radius}_grass\",\n",
    "            f\"wc{radius}_cropland\", f\"wc{radius}_builtup\", f\"wc{radius}_water\"\n",
    "        ]\n",
    "        habitat_matrix = df[habitat_cols].values + 1e-6\n",
    "        habitat_matrix = habitat_matrix / habitat_matrix.sum(axis=1, keepdims=True)\n",
    "        shannon = -np.sum(habitat_matrix * np.log(habitat_matrix), axis=1)\n",
    "\n",
    "        df[f\"habitat_diversity_{radius}\"] = shannon\n",
    "        df[f\"habitat_richness_{radius}\"] = (df[habitat_cols] > 0.05).sum(axis=1)\n",
    "        df[f\"habitat_dominance_{radius}\"] = df[habitat_cols].max(axis=1)\n",
    "\n",
    "    # Cross-scale gradients\n",
    "    df[\"vegetation_gradient_10_50\"] = df[\"vegetation_total_10\"] - df[\"vegetation_total_50\"]\n",
    "    df[\"vegetation_gradient_50_250\"] = df[\"vegetation_total_50\"] - df[\"vegetation_total_250\"]\n",
    "    df[\"urban_gradient_10_50\"] = df[\"wc10_builtup\"] - df[\"wc50_builtup\"]\n",
    "    df[\"urban_gradient_50_250\"] = df[\"wc50_builtup\"] - df[\"wc250_builtup\"]\n",
    "    df[\"water_gradient_10_100\"] = df[\"wc10_water\"] - df[\"wc100_water\"]\n",
    "    df[\"tree_gradient_10_100\"] = df[\"wc10_tree\"] - df[\"wc100_tree\"]\n",
    "\n",
    "    # Weather √ó habitat interactions\n",
    "    df[\"temp_x_vegetation_50\"] = df[\"temp_c\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"temp_x_builtup_50\"] = df[\"temp_c\"] * df[\"wc50_builtup\"]\n",
    "    df[\"temp_x_water_50\"] = df[\"temp_c\"] * df[\"wc50_water\"]\n",
    "    df[\"humidity_x_vegetation_50\"] = df[\"rhum\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"humidity_x_wetland_50\"] = df[\"rhum\"] * df[\"wc50_herb_wetland\"]\n",
    "    df[\"wind_x_vegetation_100\"] = df[\"wspd_ms\"] * df[\"vegetation_total_100\"]\n",
    "    df[\"wind_x_tree_shelter\"] = df[\"wspd_ms\"] * df[\"wc100_tree\"]\n",
    "    df[\"solar_x_vegetation\"] = df[\"swrad\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"solar_x_builtup\"] = df[\"swrad\"] * df[\"wc50_builtup\"]\n",
    "    df[\"vpd_x_vegetation\"] = df[\"vpd_kpa\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"vpd_x_water_proximity\"] = df[\"vpd_kpa\"] * (1 - df[\"wc50_water\"])\n",
    "\n",
    "    # Urban context\n",
    "    df[\"urban_heat_index\"] = df[\"wc50_builtup\"] * 2 + df[\"wc250_builtup\"] - 0.5 * df[\"vegetation_total_50\"]\n",
    "    df[\"floral_resources\"] = (\n",
    "        df[\"wc10_grass\"] * 0.5 + df[\"wc50_grass\"] * 1.0 +\n",
    "        df[\"wc50_shrub\"] * 1.5 + df[\"wc50_cropland\"] * 0.8\n",
    "    )\n",
    "    df[\"cavity_nesting_habitat\"] = df[\"wc50_tree\"] + df[\"wc50_builtup\"] * 0.2\n",
    "    df[\"ground_nesting_habitat\"] = df[\"wc10_grass\"] + df[\"wc10_bare\"] * 0.5\n",
    "    df[\"habitat_edges_50\"] = df[\"habitat_richness_50\"] * df[\"habitat_diversity_50\"]\n",
    "\n",
    "    # Temporal √ó habitat interactions\n",
    "    df[\"spring_x_vegetation\"] = df[\"is_spring\"] * df[\"vegetation_total_50\"]\n",
    "    df[\"summer_x_water\"] = df[\"is_summer\"] * df[\"wc50_water\"]\n",
    "    df[\"morning_x_flowers\"] = (df[\"hour_local\"] < 12).astype(int) * df[\"floral_resources\"]\n",
    "    df[\"afternoon_x_flowers\"] = (df[\"hour_local\"] >= 12).astype(int) * df[\"floral_resources\"]\n",
    "\n",
    "    # Spatial precision\n",
    "    df[\"log_gps_accuracy\"] = np.log1p(df[\"final_accuracy_m\"])\n",
    "    df[\"is_precise\"] = (df[\"final_accuracy_m\"] <= 10).astype(int)\n",
    "\n",
    "    return df\n",
    "\n",
    "metadata_engineered = engineer_hsi_features(metadata_for_hsi)\n",
    "\n",
    "# Ensure we only use the trained feature set\n",
    "missing_feats = [f for f in feature_list if f not in metadata_engineered.columns]\n",
    "if missing_feats:\n",
    "    raise KeyError(f\"Missing engineered features (first 10): {missing_feats[:10]}\")\n",
    "\n",
    "X_hsi = metadata_engineered[feature_list].fillna(0).to_numpy()\n",
    "hsi_probs = hsi_model.predict_proba(X_hsi)\n",
    "print(\"HSI probs:\", hsi_probs.shape)\n",
    "\n",
    "# y_true in HSI index-space\n",
    "y_true = np.array([species_to_idx[s] for s in matched_df[\"species\"].tolist()], dtype=int)\n",
    "assert hsi_probs.shape[0] == len(y_true)\n",
    "assert hsi_probs.shape[1] == NUM_CLASSES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4) Helpers (metrics, plotting, McNemar, ResNet prediction)\n",
    "\n",
    "ResNet outputs are re-ordered to the **HSI index order** before fusion, to guarantee the elementwise product is aligned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models\n",
    "from sklearn.metrics import confusion_matrix, log_loss\n",
    "\n",
    "# Optional: McNemar p-values\n",
    "try:\n",
    "    from scipy.stats import chi2\n",
    "    SCIPY_AVAILABLE = True\n",
    "except Exception:\n",
    "    SCIPY_AVAILABLE = False\n",
    "\n",
    "def topk_accuracy(y_true: np.ndarray, probs: np.ndarray, k: int) -> float:\n",
    "    k = int(min(k, probs.shape[1]))\n",
    "    topk = np.argsort(probs, axis=1)[:, -k:]\n",
    "    return float(np.mean([yt in row for yt, row in zip(y_true, topk)]))\n",
    "\n",
    "def multiclass_brier_score(y_true: np.ndarray, probs: np.ndarray) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    probs = np.asarray(probs, dtype=float)\n",
    "    n, k = probs.shape\n",
    "    onehot = np.zeros((n, k), dtype=float)\n",
    "    onehot[np.arange(n), y_true] = 1.0\n",
    "    return float(np.mean((probs - onehot) ** 2))\n",
    "\n",
    "def ece_toplabel(y_true: np.ndarray, probs: np.ndarray, n_bins: int = 15) -> float:\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    probs = np.asarray(probs, dtype=float)\n",
    "    conf = probs.max(axis=1)\n",
    "    pred = probs.argmax(axis=1)\n",
    "    acc = (pred == y_true).astype(float)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        lo, hi = bins[b], bins[b + 1]\n",
    "        mask = (conf >= lo) & (conf < hi) if b < n_bins - 1 else (conf >= lo) & (conf <= hi)\n",
    "        if not np.any(mask):\n",
    "            continue\n",
    "        ece += mask.mean() * abs(acc[mask].mean() - conf[mask].mean())\n",
    "    return float(ece)\n",
    "\n",
    "def mcnemar_test(y_true: np.ndarray, y_a: np.ndarray, y_b: np.ndarray):\n",
    "    \"\"\"Paired McNemar test between predictions y_a (ResNet) and y_b (Fused).\"\"\"\n",
    "    a_correct = (y_a == y_true)\n",
    "    b_correct = (y_b == y_true)\n",
    "\n",
    "    n11 = int(np.sum(a_correct & b_correct))\n",
    "    n10 = int(np.sum(a_correct & ~b_correct))\n",
    "    n01 = int(np.sum(~a_correct & b_correct))\n",
    "    n00 = int(np.sum(~a_correct & ~b_correct))\n",
    "\n",
    "    if (n01 + n10) > 0:\n",
    "        chi2_cc = (abs(n01 - n10) - 1) ** 2 / (n01 + n10)\n",
    "    else:\n",
    "        chi2_cc = 0.0\n",
    "\n",
    "    p_val = float(chi2.sf(chi2_cc, df=1)) if (SCIPY_AVAILABLE and (n01 + n10) > 0) else float(\"nan\")\n",
    "    return n11, n10, n01, n00, float(chi2_cc), p_val\n",
    "\n",
    "# ---------- plotting (seaborn heatmap) ----------\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def _pretty_class_name(name: str) -> str:\n",
    "    return name.replace(\"_\", \" \")\n",
    "\n",
    "def save_confusion_matrix_png(\n",
    "    cm: np.ndarray,\n",
    "    class_names: list[str],\n",
    "    out_path: Path,\n",
    "    normalize: bool = False,\n",
    "    title: str | None = None,\n",
    "    *,\n",
    "    cmap: str = \"Blues\",\n",
    "    annot_fontsize: int = 12,\n",
    "    row_norm_colours: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"Save confusion matrix plot.\n",
    "\n",
    "    - If normalize=False: annotate with integer counts.\n",
    "      Colours are row-normalised by default (like your thesis figure) so rows sum to 1 visually.\n",
    "    - If normalize=True: annotate with row-normalised percentages and colours match those percentages.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    row_norm_colours : bool\n",
    "        If True and normalize=False, colours use row-normalised values while annotations show counts.\n",
    "    \"\"\"\n",
    "    cm = np.asarray(cm, dtype=float)\n",
    "\n",
    "    # Row-normalised matrix for colouring\n",
    "    row_sums = cm.sum(axis=1, keepdims=True)\n",
    "    cm_norm = cm / np.maximum(row_sums, 1e-12)\n",
    "\n",
    "    if normalize:\n",
    "        data = cm_norm\n",
    "        annot = cm_norm\n",
    "        fmt = \".1%\"\n",
    "        cbar_label = \"Row-normalised proportion\"\n",
    "    else:\n",
    "        data = cm_norm if row_norm_colours else cm\n",
    "        annot = cm.astype(int)\n",
    "        fmt = \"d\"\n",
    "        cbar_label = \"Row-normalised proportion\" if row_norm_colours else \"Count\"\n",
    "\n",
    "    tick_labels = [_pretty_class_name(c) for c in class_names]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        data,\n",
    "        ax=ax,\n",
    "        cmap=cmap,\n",
    "        vmin=0.0,\n",
    "        vmax=1.0 if (normalize or row_norm_colours) else None,\n",
    "        square=True,\n",
    "        annot=annot,\n",
    "        fmt=fmt,\n",
    "        annot_kws={\"size\": annot_fontsize},\n",
    "        linewidths=0.5,\n",
    "        linecolor=\"white\",\n",
    "        cbar_kws={\"label\": cbar_label},\n",
    "        xticklabels=tick_labels,\n",
    "        yticklabels=tick_labels,\n",
    "    )\n",
    "\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_title(title or (\"Confusion matrix (row-normalised)\" if normalize else \"Confusion matrix (counts annotated; colours row-normalised)\"))\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "# ---------- dataset for matched image paths ----------\n",
    "class ImagePathDataset(Dataset):\n",
    "    def __init__(self, image_paths: list[str], transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        from PIL import Image\n",
    "        p = self.image_paths[idx]\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        return x, idx  # idx to keep ordering\n",
    "\n",
    "def load_resnet18(num_classes: int, weights_path: Path, device: torch.device) -> nn.Module:\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "@torch.no_grad()\n",
    "def resnet_predict_proba_in_hsi_order(\n",
    "    weights_path: Path,\n",
    "    image_paths: list[str],\n",
    "    transform,\n",
    "    device: torch.device,\n",
    "    resnet_class_species: list[str],\n",
    "    species_to_idx: dict[str, int],\n",
    "    temperature: float | None = None,\n",
    "    batch_size: int = 32,\n",
    "):\n",
    "    \"\"\"Return probabilities in HSI index order (columns aligned to species_mapping idx).\"\"\"\n",
    "    model = load_resnet18(NUM_CLASSES, weights_path, device)\n",
    "\n",
    "    ds = ImagePathDataset(image_paths, transform)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    probs_resnet_order = np.zeros((len(image_paths), NUM_CLASSES), dtype=np.float32)\n",
    "\n",
    "    for xb, idxs in dl:\n",
    "        xb = xb.to(device)\n",
    "        logits = model(xb)\n",
    "        if temperature is not None:\n",
    "            logits = logits / float(temperature)\n",
    "        probs = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "        probs_resnet_order[idxs.numpy()] = probs\n",
    "\n",
    "    # Map ResNet column order -> HSI idx order\n",
    "    # resnet_class_species: list in resnet/ImageFolder class order\n",
    "    perm = [species_to_idx[s] for s in resnet_class_species]  # length NUM_CLASSES\n",
    "    probs_hsi_order = np.zeros_like(probs_resnet_order)\n",
    "    for resnet_col, hsi_col in enumerate(perm):\n",
    "        probs_hsi_order[:, hsi_col] = probs_resnet_order[:, resnet_col]\n",
    "\n",
    "    return probs_hsi_order\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5) Run fusion across all 10 ResNet models\n",
    "\n",
    "This will:\n",
    "- load each `models/vision/*.pth`\n",
    "- load matching temperature file if present (`models/vision/temperatures/temperature_<run_tag>.npy`)\n",
    "- compute ResNet probs, fuse with HSI probs, compute metrics + McNemar\n",
    "- save tables/plots to `outputs/fusion_poe/...`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Rescued vs Hurt analysis\n",
    "\n",
    "To make the fusion behaviour interpretable, we decompose the net Top-1 accuracy change into:\n",
    "\n",
    "- **Rescued:** ResNet wrong but fused correct  \n",
    "- **Hurt:** ResNet correct but fused wrong  \n",
    "\n",
    "For each run we report rescued/hurt counts and rates overall, and also aggregated per species (true label).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Identify models (only ResNet run checkpoints)\n",
    "model_files = sorted([p for p in VISION_MODEL_DIR.glob(\"*.pth\") if p.name.startswith(\"resnet18_run_\")])\n",
    "print(f\"Found {len(model_files)} vision checkpoints in {VISION_MODEL_DIR}\")\n",
    "\n",
    "if len(model_files) == 0:\n",
    "    raise FileNotFoundError(f\"No resnet checkpoints found in {VISION_MODEL_DIR}\")\n",
    "\n",
    "# resnet class order: from ImageFolder class order (same as training if folder names match)\n",
    "resnet_class_species = [c.replace(\"_\", \" \") for c in test_dataset.classes]\n",
    "\n",
    "# Matched image paths in the order used for pairing\n",
    "image_paths = matched_df[\"image_path\"].tolist()\n",
    "\n",
    "all_results = []\n",
    "mcnemar_rows = []\n",
    "species_improvements = {sp: [] for sp in WILD_SPECIES}\n",
    "\n",
    "# Rescued / Hurt analysis (per-run + per-species)\n",
    "rescue_rows = []\n",
    "rescue_species_rows = []\n",
    "species_rescued = {sp: [] for sp in WILD_SPECIES}\n",
    "species_hurt = {sp: [] for sp in WILD_SPECIES}\n",
    "\n",
    "all_resnet_preds = []\n",
    "all_fused_preds = []\n",
    "run_tags = []  # in the same order as model_files\n",
    "\n",
    "# Optional: save full probabilities (can be large)\n",
    "SAVE_FULL_PROBS = False\n",
    "\n",
    "for i, model_path in enumerate(model_files, 1):\n",
    "    run_tag = model_path.stem  # e.g. resnet18_run_10_seed_51\n",
    "    run_tags.append(run_tag)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"MODEL {i}/{len(model_files)}: {run_tag}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Temperature \n",
    "    t_path = VISION_TEMPS_DIR / f\"temperature_{run_tag}.npy\"\n",
    "    temperature = float(np.load(t_path).ravel()[0]) if t_path.exists() else None\n",
    "    if temperature is not None:\n",
    "        print(f\"  üå°Ô∏è  Using temperature scaling T*={temperature:.4f}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è  No temperature file found -> uncalibrated logits\")\n",
    "\n",
    "    # ResNet probabilities (aligned to HSI idx order)\n",
    "    resnet_probs = resnet_predict_proba_in_hsi_order(\n",
    "        weights_path=model_path,\n",
    "        image_paths=image_paths,\n",
    "        transform=test_transform,\n",
    "        device=DEVICE,\n",
    "        resnet_class_species=resnet_class_species,\n",
    "        species_to_idx=species_to_idx,\n",
    "        temperature=temperature,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    # Fusion (PoE)\n",
    "    fused_probs = resnet_probs * hsi_probs\n",
    "    fused_probs = fused_probs / np.maximum(fused_probs.sum(axis=1, keepdims=True), 1e-12)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred_resnet = resnet_probs.argmax(axis=1)\n",
    "    y_pred_fused = fused_probs.argmax(axis=1)\n",
    "\n",
    "    all_resnet_preds.append(y_pred_resnet)\n",
    "    all_fused_preds.append(y_pred_fused)\n",
    "\n",
    "    # Rescued / Hurt (per-sample)\n",
    "    res_correct = (y_pred_resnet == y_true)\n",
    "    fus_correct = (y_pred_fused == y_true)\n",
    "    rescued_mask = (~res_correct) & fus_correct\n",
    "    hurt_mask = res_correct & (~fus_correct)\n",
    "\n",
    "    n_rescued = int(rescued_mask.sum())\n",
    "    n_hurt = int(hurt_mask.sum())\n",
    "    n_total = int(len(y_true))\n",
    "    rescue_rows.append({\n",
    "        \"run_tag\": run_tag,\n",
    "        \"rescued_n\": n_rescued,\n",
    "        \"hurt_n\": n_hurt,\n",
    "        \"net_rescue_n\": n_rescued - n_hurt,\n",
    "        \"rescued_rate\": n_rescued / n_total if n_total else np.nan,\n",
    "        \"hurt_rate\": n_hurt / n_total if n_total else np.nan,\n",
    "    })\n",
    "\n",
    "    # Per-species rescued / hurt (Top-1 only)\n",
    "    species_arr = matched_df[\"species\"].to_numpy()\n",
    "    for sp in WILD_SPECIES:\n",
    "        idxs = np.where(species_arr == sp)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        r_sp = int(rescued_mask[idxs].sum())\n",
    "        h_sp = int(hurt_mask[idxs].sum())\n",
    "        species_rescued[sp].append(r_sp)\n",
    "        species_hurt[sp].append(h_sp)\n",
    "        rescue_species_rows.append({\n",
    "            \"run_tag\": run_tag,\n",
    "            \"species\": sp,\n",
    "            \"n_samples\": int(len(idxs)),\n",
    "            \"rescued_n\": r_sp,\n",
    "            \"hurt_n\": h_sp,\n",
    "            \"net_rescue_n\": r_sp - h_sp,\n",
    "            \"rescued_rate\": r_sp / len(idxs),\n",
    "            \"hurt_rate\": h_sp / len(idxs),\n",
    "        })\n",
    "\n",
    "\n",
    "    # Metrics\n",
    "    res_top1 = topk_accuracy(y_true, resnet_probs, 1)\n",
    "    res_top3 = topk_accuracy(y_true, resnet_probs, 3)\n",
    "    res_top5 = topk_accuracy(y_true, resnet_probs, 5)\n",
    "\n",
    "    hsi_top1 = topk_accuracy(y_true, hsi_probs, 1)\n",
    "\n",
    "    fus_top1 = topk_accuracy(y_true, fused_probs, 1)\n",
    "    fus_top3 = topk_accuracy(y_true, fused_probs, 3)\n",
    "    fus_top5 = topk_accuracy(y_true, fused_probs, 5)\n",
    "\n",
    "    imp_top1 = (fus_top1 - res_top1) * 100.0\n",
    "\n",
    "    # Calibration metrics (ResNet vs Fused)\n",
    "    res_brier = multiclass_brier_score(y_true, resnet_probs)\n",
    "    fus_brier = multiclass_brier_score(y_true, fused_probs)\n",
    "    res_ece = ece_toplabel(y_true, resnet_probs, n_bins=15)\n",
    "    fus_ece = ece_toplabel(y_true, fused_probs, n_bins=15)\n",
    "    res_ll = float(log_loss(y_true, resnet_probs))\n",
    "    fus_ll = float(log_loss(y_true, fused_probs))\n",
    "\n",
    "    # McNemar\n",
    "    n11, n10, n01, n00, chi2_cc, p_val = mcnemar_test(y_true, y_pred_resnet, y_pred_fused)\n",
    "\n",
    "    print(f\"  ResNet Top-1: {res_top1:.1%} | Fused Top-1: {fus_top1:.1%} | Œî={imp_top1:+.2f} pp\")\n",
    "    print(f\"  ECE: {res_ece:.4f} -> {fus_ece:.4f} | Brier: {res_brier:.4f} -> {fus_brier:.4f} | LogLoss: {res_ll:.4f} -> {fus_ll:.4f}\")\n",
    "    if not np.isnan(p_val):\n",
    "        print(f\"  McNemar: chi2_cc={chi2_cc:.3f}, p={p_val:.3e} (n01={n01}, n10={n10})\")\n",
    "    else:\n",
    "        print(f\"  McNemar: chi2_cc={chi2_cc:.3f} (p-value unavailable) (n01={n01}, n10={n10})\")\n",
    "\n",
    "    # Per-species improvements (Top-1)\n",
    "    for sp in WILD_SPECIES:\n",
    "        idxs = np.where(matched_df[\"species\"].to_numpy() == sp)[0]\n",
    "        if len(idxs) == 0:\n",
    "            continue\n",
    "        r = topk_accuracy(y_true[idxs], resnet_probs[idxs], 1)\n",
    "        f = topk_accuracy(y_true[idxs], fused_probs[idxs], 1)\n",
    "        species_improvements[sp].append((f - r) * 100.0)\n",
    "\n",
    "    # Save per-run outputs\n",
    "    run_metrics_dir = MET_DIR / run_tag\n",
    "    run_fig_dir = FIG_DIR / run_tag\n",
    "    run_pred_dir = PRD_DIR / run_tag\n",
    "    run_metrics_dir.mkdir(parents=True, exist_ok=True)\n",
    "    run_fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "    run_pred_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    cm_res = confusion_matrix(y_true, y_pred_resnet, labels=list(range(NUM_CLASSES)))\n",
    "    cm_fus = confusion_matrix(y_true, y_pred_fused, labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "    np.save(run_metrics_dir / f\"confusion_matrix_resnet_{run_tag}.npy\", cm_res)\n",
    "    np.save(run_metrics_dir / f\"confusion_matrix_fused_{run_tag}.npy\", cm_fus)\n",
    "\n",
    "    class_names = [idx_to_species[i] for i in range(NUM_CLASSES)]\n",
    "    save_confusion_matrix_png(cm_res, class_names, run_fig_dir / f\"confusion_matrix_resnet_{run_tag}_counts.png\", normalize=False,\n",
    "                              title=f\"ResNet confusion (counts) ‚Äì {run_tag}\")\n",
    "    save_confusion_matrix_png(cm_res, class_names, run_fig_dir / f\"confusion_matrix_resnet_{run_tag}_norm.png\", normalize=True,\n",
    "                              title=f\"ResNet confusion (normalized) ‚Äì {run_tag}\")\n",
    "    save_confusion_matrix_png(cm_fus, class_names, run_fig_dir / f\"confusion_matrix_fused_{run_tag}_counts.png\", normalize=False,\n",
    "                              title=f\"Fused confusion (counts) ‚Äì {run_tag}\")\n",
    "    save_confusion_matrix_png(cm_fus, class_names, run_fig_dir / f\"confusion_matrix_fused_{run_tag}_norm.png\", normalize=True,\n",
    "                              title=f\"Fused confusion (normalized) ‚Äì {run_tag}\")\n",
    "\n",
    "    # Prediction table (essential columns)\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"image_path\": [str(Path(p).resolve().relative_to(REPO_ROOT)) if str(p).startswith(str(REPO_ROOT)) else str(p) for p in image_paths],\n",
    "        \"species\": matched_df[\"species\"],\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred_resnet\": y_pred_resnet,\n",
    "        \"y_pred_fused\": y_pred_fused,\n",
    "        \"resnet_conf\": resnet_probs.max(axis=1),\n",
    "        \"fused_conf\": fused_probs.max(axis=1),\n",
    "    })\n",
    "    pred_df.to_csv(run_pred_dir / f\"predictions_{run_tag}.csv\", index=False)\n",
    "\n",
    "    if SAVE_FULL_PROBS:\n",
    "        np.save(run_pred_dir / f\"resnet_probs_{run_tag}.npy\", resnet_probs.astype(np.float32))\n",
    "        np.save(run_pred_dir / f\"fused_probs_{run_tag}.npy\", fused_probs.astype(np.float32))\n",
    "\n",
    "    all_results.append({\n",
    "        \"run_tag\": run_tag,\n",
    "        \"temperature_T\": temperature if temperature is not None else np.nan,\n",
    "        \"resnet_top1\": res_top1,\n",
    "        \"resnet_top3\": res_top3,\n",
    "        \"resnet_top5\": res_top5,\n",
    "        \"hsi_top1\": hsi_top1,\n",
    "        \"fused_top1\": fus_top1,\n",
    "        \"fused_top3\": fus_top3,\n",
    "        \"fused_top5\": fus_top5,\n",
    "        \"improvement_top1_pp\": imp_top1,\n",
    "        \"rescued_n\": n_rescued,\n",
    "        \"hurt_n\": n_hurt,\n",
    "        \"net_rescue_n\": (n_rescued - n_hurt),\n",
    "        \"rescued_rate\": (n_rescued / len(y_true)) if len(y_true) else np.nan,\n",
    "        \"hurt_rate\": (n_hurt / len(y_true)) if len(y_true) else np.nan,\n",
    "        \"resnet_ece\": res_ece,\n",
    "        \"fused_ece\": fus_ece,\n",
    "        \"resnet_brier\": res_brier,\n",
    "        \"fused_brier\": fus_brier,\n",
    "        \"resnet_logloss\": res_ll,\n",
    "        \"fused_logloss\": fus_ll,\n",
    "    })\n",
    "\n",
    "    mcnemar_rows.append({\n",
    "        \"run_tag\": run_tag,\n",
    "        \"n11_both_correct\": n11,\n",
    "        \"n10_resnet_only\": n10,\n",
    "        \"n01_fused_only\": n01,\n",
    "        \"n00_both_wrong\": n00,\n",
    "        \"chi2_cc\": chi2_cc,\n",
    "        \"p_value\": p_val,\n",
    "    })\n",
    "\n",
    "# Aggregate tables\n",
    "results_df = pd.DataFrame(all_results).sort_values(\"run_tag\").reset_index(drop=True)\n",
    "mcnemar_df = pd.DataFrame(mcnemar_rows).sort_values(\"run_tag\").reset_index(drop=True)\n",
    "\n",
    "results_df.to_csv(MET_DIR / \"fusion_results_all_models_with_calibration.csv\", index=False)\n",
    "mcnemar_df.to_csv(MET_DIR / \"fusion_mcnemar_all_models.csv\", index=False)\n",
    "\n",
    "# Rescued / Hurt summary tables\n",
    "rescue_df = pd.DataFrame(rescue_rows).sort_values(\"run_tag\").reset_index(drop=True)\n",
    "rescue_species_df = pd.DataFrame(rescue_species_rows).sort_values([\"species\", \"run_tag\"]).reset_index(drop=True)\n",
    "\n",
    "rescue_df.to_csv(MET_DIR / \"fusion_rescued_hurt_all_models.csv\", index=False)\n",
    "rescue_species_df.to_csv(MET_DIR / \"fusion_rescued_hurt_per_species.csv\", index=False)\n",
    "\n",
    "# Quick printed summary (mean ¬± std across runs)\n",
    "def _mean_std(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(np.nanmean(x)), float(np.nanstd(x))\n",
    "\n",
    "m_r, s_r = _mean_std(rescue_df[\"rescued_rate\"].values)\n",
    "m_h, s_h = _mean_std(rescue_df[\"hurt_rate\"].values)\n",
    "m_n, s_n = _mean_std(rescue_df[\"net_rescue_n\"].values)\n",
    "\n",
    "print(\"\\nRescued/Hurt (mean ¬± std across runs):\")\n",
    "print(f\"  rescued_rate: {m_r:.4f} ¬± {s_r:.4f}\")\n",
    "print(f\"  hurt_rate   : {m_h:.4f} ¬± {s_h:.4f}\")\n",
    "print(f\"  net_rescue_n: {m_n:.2f} ¬± {s_n:.2f}\")\n",
    "\n",
    "print(\"\\nPer-species rescued/hurt (mean ¬± std counts across runs):\")\n",
    "for sp in WILD_SPECIES:\n",
    "    r = np.asarray(species_rescued[sp], dtype=float)\n",
    "    h = np.asarray(species_hurt[sp], dtype=float)\n",
    "    if len(r) == 0:\n",
    "        continue\n",
    "    print(f\"  {sp:<30} rescued {r.mean():6.2f} ¬± {r.std():5.2f} | hurt {h.mean():6.2f} ¬± {h.std():5.2f}\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"-\", MET_DIR / \"fusion_results_all_models_with_calibration.csv\")\n",
    "print(\"-\", MET_DIR / \"fusion_mcnemar_all_models.csv\")\n",
    "\n",
    "# McNemar summary\n",
    "valid_p = mcnemar_df[\"p_value\"].dropna()\n",
    "if len(valid_p) > 0:\n",
    "    num_sig = int((valid_p < 0.05).sum())\n",
    "    mean_chi2 = float(mcnemar_df[\"chi2_cc\"].mean())\n",
    "    mean_delta = float((mcnemar_df[\"n01_fused_only\"] - mcnemar_df[\"n10_resnet_only\"]).mean())\n",
    "    print(f\"\\nMcNemar significant (p<0.05): {num_sig}/{len(valid_p)}\")\n",
    "    print(f\"Mean McNemar chi2_cc: {mean_chi2:.3f}\")\n",
    "    print(f\"Mean (fused-only correct - resnet-only correct): {mean_delta:.2f}\")\n",
    "else:\n",
    "    print(\"\\nMcNemar p-values not available (SciPy not installed or no discordant pairs).\")\n",
    "\n",
    "\n",
    "# Mean confusion matrices\n",
    "all_resnet_preds = np.stack(all_resnet_preds, axis=0)  # (n_models, n_samples)\n",
    "all_fused_preds = np.stack(all_fused_preds, axis=0)\n",
    "\n",
    "cm_res_sum = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=float)\n",
    "cm_fus_sum = np.zeros((NUM_CLASSES, NUM_CLASSES), dtype=float)\n",
    "for i in range(all_resnet_preds.shape[0]):\n",
    "    cm_res_sum += confusion_matrix(y_true, all_resnet_preds[i], labels=list(range(NUM_CLASSES)))\n",
    "    cm_fus_sum += confusion_matrix(y_true, all_fused_preds[i], labels=list(range(NUM_CLASSES)))\n",
    "\n",
    "cm_res_mean = cm_res_sum / all_resnet_preds.shape[0]\n",
    "cm_fus_mean = cm_fus_sum / all_fused_preds.shape[0]\n",
    "\n",
    "np.save(MET_DIR / \"confusion_matrix_resnet_MEAN_10runs.npy\", cm_res_mean)\n",
    "np.save(MET_DIR / \"confusion_matrix_fused_MEAN_10runs.npy\", cm_fus_mean)\n",
    "\n",
    "class_names = [idx_to_species[i] for i in range(NUM_CLASSES)]\n",
    "save_confusion_matrix_png(cm_res_mean, class_names, FIG_DIR / \"confusion_matrix_resnet_MEAN_10runs_norm.png\", normalize=True,\n",
    "                          title=\"Mean confusion (normalized) ‚Äì ResNet (10 runs)\")\n",
    "save_confusion_matrix_png(cm_fus_mean, class_names, FIG_DIR / \"confusion_matrix_fused_MEAN_10runs_norm.png\", normalize=True,\n",
    "                          title=\"Mean confusion (normalized) ‚Äì Fused (10 runs)\")\n",
    "\n",
    "print(\"-\", FIG_DIR / \"confusion_matrix_resnet_MEAN_10runs_norm.png\")\n",
    "print(\"-\", FIG_DIR / \"confusion_matrix_fused_MEAN_10runs_norm.png\")\n",
    "\n",
    "# Short printed summary\n",
    "def _mean_std(x): \n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(np.nanmean(x)), float(np.nanstd(x))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY (mean ¬± std across runs)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for col in [\"resnet_top1\", \"fused_top1\", \"improvement_top1_pp\", \"resnet_ece\", \"fused_ece\", \"resnet_brier\", \"fused_brier\", \"resnet_logloss\", \"fused_logloss\"]:\n",
    "    m, s = _mean_std(results_df[col].values)\n",
    "    print(f\"{col:>22}: {m:.4f} ¬± {s:.4f}\")\n",
    "\n",
    "print(\"\\nPer-species Top-1 improvement (mean ¬± std pp):\")\n",
    "for sp in WILD_SPECIES:\n",
    "    arr = np.asarray(species_improvements[sp], dtype=float)\n",
    "    if len(arr) == 0:\n",
    "        continue\n",
    "    print(f\"  {sp:<30} {arr.mean():+6.2f} ¬± {arr.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6) Optional quick figures\n",
    "\n",
    "Two simple figures that are commonly useful in the thesis:\n",
    "- distribution of Top-1 improvements across runs\n",
    "- per-species mean improvement (mean ¬± std across runs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1) Improvement distribution\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.hist(results_df[\"improvement_top1_pp\"], bins=10, edgecolor=\"black\", alpha=0.7)\n",
    "ax.axvline(results_df[\"improvement_top1_pp\"].mean(), linestyle=\"--\", linewidth=2,\n",
    "           label=f\"Mean: {results_df['improvement_top1_pp'].mean():.2f} pp\")\n",
    "ax.set_xlabel(\"Top-1 improvement (percentage points)\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "ax.set_title(\"Fusion improvement across 10 ResNet runs\")\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"fusion_improvement_hist.png\", dpi=250)\n",
    "plt.show()\n",
    "\n",
    "# 2) Per-species improvement bars\n",
    "species = WILD_SPECIES\n",
    "means = [np.mean(species_improvements[s]) if len(species_improvements[s]) else 0.0 for s in species]\n",
    "stds  = [np.std(species_improvements[s]) if len(species_improvements[s]) else 0.0 for s in species]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "x = np.arange(len(species))\n",
    "ax.bar(x, means, yerr=stds, capsize=4, alpha=0.7, edgecolor=\"black\")\n",
    "ax.axhline(0, linewidth=1)\n",
    "ax.set_xticks(x, labels=[s.split()[-1] for s in species], rotation=45, ha=\"right\")\n",
    "ax.set_ylabel(\"Top-1 improvement (pp)\")\n",
    "ax.set_title(\"Per-species fusion improvement (mean ¬± std across runs)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(FIG_DIR / \"fusion_improvement_per_species.png\", dpi=250)\n",
    "plt.show()\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"-\", FIG_DIR / \"fusion_improvement_hist.png\")\n",
    "print(\"-\", FIG_DIR / \"fusion_improvement_per_species.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "# --- prerequisites ---\n",
    "species_arr = matched_df[\"species\"].to_numpy()\n",
    "y_true_arr = np.asarray(y_true, dtype=int)\n",
    "\n",
    "all_resnet_preds_arr = np.asarray(all_resnet_preds)  # (n_runs, n_samples)\n",
    "all_fused_preds_arr  = np.asarray(all_fused_preds)   # (n_runs, n_samples)\n",
    "\n",
    "assert all_resnet_preds_arr.shape == all_fused_preds_arr.shape\n",
    "assert all_resnet_preds_arr.shape[1] == len(y_true_arr) == len(species_arr)\n",
    "\n",
    "species_list = list(pd.unique(species_arr))\n",
    "\n",
    "# -----------------------------\n",
    "# A) Per-species Top-1 per run\n",
    "# -----------------------------\n",
    "rows = []\n",
    "for run_tag, y_r, y_f in zip(run_tags, all_resnet_preds_arr, all_fused_preds_arr):\n",
    "    for sp in species_list:\n",
    "        idx = np.where(species_arr == sp)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "\n",
    "        r_acc = float(np.mean(y_r[idx] == y_true_arr[idx]))\n",
    "        f_acc = float(np.mean(y_f[idx] == y_true_arr[idx]))\n",
    "        delta_pp = 100.0 * (f_acc - r_acc)\n",
    "\n",
    "        rows.append({\n",
    "            \"run_tag\": run_tag,\n",
    "            \"species\": sp,\n",
    "            \"n\": int(len(idx)),\n",
    "            \"cnn_acc\": r_acc,          # in [0,1]\n",
    "            \"fused_acc\": f_acc,        # in [0,1]\n",
    "            \"delta_pp\": delta_pp,      # percentage points\n",
    "        })\n",
    "\n",
    "per_run_species_df = pd.DataFrame(rows)\n",
    "\n",
    "# -----------------------------------\n",
    "# B) Rescued/Hurt per species per run\n",
    "# -----------------------------------\n",
    "rh_rows = []\n",
    "for run_tag, y_r, y_f in zip(run_tags, all_resnet_preds_arr, all_fused_preds_arr):\n",
    "    res_correct = (y_r == y_true_arr)\n",
    "    fus_correct = (y_f == y_true_arr)\n",
    "\n",
    "    rescued = (~res_correct) & fus_correct\n",
    "    hurt    = res_correct & (~fus_correct)\n",
    "\n",
    "    for sp in species_list:\n",
    "        idx = np.where(species_arr == sp)[0]\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        rh_rows.append({\n",
    "            \"run_tag\": run_tag,\n",
    "            \"species\": sp,\n",
    "            \"n\": int(len(idx)),\n",
    "            \"rescued_n\": int(rescued[idx].sum()),\n",
    "            \"hurt_n\": int(hurt[idx].sum()),\n",
    "        })\n",
    "\n",
    "rescued_hurt_df = pd.DataFrame(rh_rows)\n",
    "\n",
    "# Save the raw per-run tables (handy for debugging / appendix)\n",
    "per_run_species_df.to_csv(MET_DIR / \"per_species_top1_by_run.csv\", index=False)\n",
    "rescued_hurt_df.to_csv(MET_DIR / \"rescued_hurt_by_run.csv\", index=False)\n",
    "\n",
    "print(\"Saved raw tables:\")\n",
    "print(\"-\", MET_DIR / \"per_species_top1_by_run.csv\")\n",
    "print(\"-\", MET_DIR / \"rescued_hurt_by_run.csv\")\n",
    "\n",
    "\n",
    "print(\"\\nPreview: per-run per-species Top-1 table (first 20 rows)\")\n",
    "display(per_run_species_df.head(20))\n",
    "\n",
    "print(\"\\nPreview: per-run rescued/hurt table (first 20 rows)\")\n",
    "display(rescued_hurt_df.head(20))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "def fmt_mean_std(x, is_percent=True):\n",
    "    m = float(np.mean(x))\n",
    "    s = float(np.std(x, ddof=1)) if len(x) > 1 else 0.0\n",
    "    if is_percent:\n",
    "        return f\"{100*m:.1f} ¬± {100*s:.1f}\"\n",
    "    return f\"{m:.3f} ¬± {s:.3f}\"\n",
    "\n",
    "# -----------------------------\n",
    "# Table: Per-species Top-1\n",
    "# -----------------------------\n",
    "g = per_run_species_df.groupby(\"species\", sort=False)\n",
    "\n",
    "table_top1 = g.agg(\n",
    "    N=(\"n\", \"first\"),\n",
    "    cnn_mean=(\"cnn_acc\", \"mean\"),\n",
    "    cnn_std=(\"cnn_acc\", \"std\"),\n",
    "    fused_mean=(\"fused_acc\", \"mean\"),\n",
    "    fused_std=(\"fused_acc\", \"std\"),\n",
    "    delta_mean=(\"delta_pp\", \"mean\"),\n",
    "    delta_std=(\"delta_pp\", \"std\"),\n",
    "    delta_min=(\"delta_pp\", \"min\"),\n",
    "    delta_max=(\"delta_pp\", \"max\"),\n",
    ").reset_index()\n",
    "\n",
    "# Relative improvement in %\n",
    "table_top1[\"rel_improv_pct\"] = 100.0 * (table_top1[\"delta_mean\"] / (100.0 * table_top1[\"cnn_mean\"]))\n",
    "\n",
    "# Pretty string columns (match your screenshot style)\n",
    "table_top1[\"CNN (%)\"]   = [f\"{100*m:.1f} ¬± {100*s:.1f}\" for m, s in zip(table_top1[\"cnn_mean\"],  table_top1[\"cnn_std\"].fillna(0))]\n",
    "table_top1[\"Fused (%)\"] = [f\"{100*m:.1f} ¬± {100*s:.1f}\" for m, s in zip(table_top1[\"fused_mean\"], table_top1[\"fused_std\"].fillna(0))]\n",
    "table_top1[\"Œî (pp)\"]    = [f\"{m:+.2f} ¬± {s:.2f}\"        for m, s in zip(table_top1[\"delta_mean\"], table_top1[\"delta_std\"].fillna(0))]\n",
    "table_top1[\"Rel. improv. (%)\"] = [f\"{x:+.1f}\" for x in table_top1[\"rel_improv_pct\"]]\n",
    "table_top1[\"Range (pp)\"] = [f\"{mn:+.2f} to {mx:+.2f}\" for mn, mx in zip(table_top1[\"delta_min\"], table_top1[\"delta_max\"])]\n",
    "\n",
    "# Sort like your table: by improvement descending\n",
    "table_top1 = table_top1.sort_values(\"delta_mean\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# Overall mean row (across runs, overall accuracy)\n",
    "# Compute per-run overall accuracies and average them (clean, matches your setup)\n",
    "overall_rows = []\n",
    "for run_tag in per_run_species_df[\"run_tag\"].unique():\n",
    "    run_block = per_run_species_df[per_run_species_df[\"run_tag\"] == run_tag]\n",
    "    # weighted by N for overall mean accuracy\n",
    "    w = run_block[\"n\"].to_numpy()\n",
    "    cnn = (run_block[\"cnn_acc\"].to_numpy() * w).sum() / w.sum()\n",
    "    fus = (run_block[\"fused_acc\"].to_numpy() * w).sum() / w.sum()\n",
    "    overall_rows.append({\"run_tag\": run_tag, \"cnn\": cnn, \"fused\": fus, \"delta_pp\": 100*(fus-cnn)})\n",
    "\n",
    "overall_df = pd.DataFrame(overall_rows)\n",
    "overall_row = {\n",
    "    \"species\": \"Mean (Overall)\",\n",
    "    \"N\": int(per_run_species_df[\"n\"].dropna().unique().sum()) if per_run_species_df[\"n\"].nunique() == 1 else int(len(y_true)),\n",
    "    \"CNN (%)\":   fmt_mean_std(overall_df[\"cnn\"].to_numpy(), is_percent=True),\n",
    "    \"Fused (%)\": fmt_mean_std(overall_df[\"fused\"].to_numpy(), is_percent=True),\n",
    "    \"Œî (pp)\":    f\"{overall_df['delta_pp'].mean():+.2f} ¬± {overall_df['delta_pp'].std(ddof=1):.2f}\",\n",
    "    \"Rel. improv. (%)\": f\"{(100*(overall_df['fused'].mean()/overall_df['cnn'].mean() - 1)):+.1f}\",\n",
    "    \"Range (pp)\": f\"{overall_df['delta_pp'].min():+.2f} to {overall_df['delta_pp'].max():+.2f}\",\n",
    "}\n",
    "\n",
    "table_top1_out = table_top1[[\"species\",\"N\",\"CNN (%)\",\"Fused (%)\",\"Œî (pp)\",\"Rel. improv. (%)\",\"Range (pp)\"]].copy()\n",
    "table_top1_out = pd.concat([table_top1_out, pd.DataFrame([overall_row])], ignore_index=True)\n",
    "\n",
    "table_top1_out.to_csv(MET_DIR / \"table_per_species_top1.csv\", index=False)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Table: Rescued / Hurt\n",
    "# -----------------------------\n",
    "g2 = rescued_hurt_df.groupby(\"species\", sort=False)\n",
    "table_rh = g2.agg(\n",
    "    N=(\"n\", \"first\"),\n",
    "    rescued_mean=(\"rescued_n\", \"mean\"),\n",
    "    hurt_mean=(\"hurt_n\", \"mean\"),\n",
    ").reset_index()\n",
    "\n",
    "table_rh[\"rescued_pct\"] = 100.0 * (table_rh[\"rescued_mean\"] / table_rh[\"N\"])\n",
    "table_rh[\"hurt_pct\"]    = 100.0 * (table_rh[\"hurt_mean\"] / table_rh[\"N\"])\n",
    "table_rh[\"net_pp\"]      = table_rh[\"rescued_pct\"] - table_rh[\"hurt_pct\"]\n",
    "\n",
    "# Format like screenshot: mean counts with (percentage)\n",
    "table_rh[\"Rescued Count (%)\"] = [f\"{c:.1f} ({p:.1f}%)\" for c, p in zip(table_rh[\"rescued_mean\"], table_rh[\"rescued_pct\"])]\n",
    "table_rh[\"Hurt Count (%)\"]    = [f\"{c:.1f} ({p:.1f}%)\" for c, p in zip(table_rh[\"hurt_mean\"], table_rh[\"hurt_pct\"])]\n",
    "table_rh[\"Net Impact\"]        = [f\"{x:+.2f} pp\" for x in table_rh[\"net_pp\"]]\n",
    "\n",
    "# Overall mean row (mean rescued/hurt totals across runs)\n",
    "totals_by_run = rescued_hurt_df.groupby(\"run_tag\").agg(\n",
    "    rescued=(\"rescued_n\",\"sum\"),\n",
    "    hurt=(\"hurt_n\",\"sum\"),\n",
    "    N=(\"n\",\"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "N_total = int(totals_by_run[\"N\"].iloc[0])\n",
    "rescued_mean = float(totals_by_run[\"rescued\"].mean())\n",
    "hurt_mean = float(totals_by_run[\"hurt\"].mean())\n",
    "rescued_pct = 100.0 * rescued_mean / N_total\n",
    "hurt_pct = 100.0 * hurt_mean / N_total\n",
    "\n",
    "overall_rh = {\n",
    "    \"species\": \"Overall Mean\",\n",
    "    \"N\": N_total,\n",
    "    \"Rescued Count (%)\": f\"{rescued_mean:.1f} ({rescued_pct:.1f}%)\",\n",
    "    \"Hurt Count (%)\": f\"{hurt_mean:.1f} ({hurt_pct:.1f}%)\",\n",
    "    \"Net Impact\": f\"{(rescued_pct - hurt_pct):+.2f} pp\",\n",
    "}\n",
    "\n",
    "table_rh_out = table_rh[[\"species\",\"N\",\"Rescued Count (%)\",\"Hurt Count (%)\",\"Net Impact\"]].copy()\n",
    "table_rh_out = pd.concat([table_rh_out, pd.DataFrame([overall_rh])], ignore_index=True)\n",
    "\n",
    "table_rh_out.to_csv(MET_DIR / \"table_rescued_hurt.csv\", index=False)\n",
    "\n",
    "print(\"Saved aggregated tables:\")\n",
    "print(\"-\", MET_DIR / \"table_per_species_top1.csv\")\n",
    "print(\"-\", MET_DIR / \"table_rescued_hurt.csv\")\n",
    "\n",
    "print(\"\\n‚úÖ Table: Per-species Top-1 (final)\")\n",
    "display(table_top1_out.style.hide(axis=\"index\"))\n",
    "\n",
    "print(\"\\n‚úÖ Table: Rescued / Hurt (final)\")\n",
    "display(table_rh_out.style.hide(axis=\"index\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis-py312)",
   "language": "python",
   "name": "thesis-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
