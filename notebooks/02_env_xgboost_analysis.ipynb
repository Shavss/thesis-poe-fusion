{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Environmental expert (XGBoost): analysis & calibration\n",
    "\n",
    "This notebook contains **diagnostics and post-hoc calibration** for the trained environmental (context) expert used in the thesis.\n",
    "\n",
    "It is designed to be run **after** the training notebook (e.g., `01_env_xgboost_final_clean.ipynb`) has produced:\n",
    "\n",
    "- `models/context/xgboost_hsi_model_FINAL_no_vespula.json`\n",
    "- `models/context/feature_names_FINAL_no_vespula.csv`\n",
    "- `models/context/species_mapping_FINAL_no_vespula.csv`\n",
    "- *(optional but recommended)* `outputs/context_xgb/preds/test_predictions_2025_FINAL_no_vespula.parquet`\n",
    "\n",
    "## What you will do here\n",
    "1. Load the trained model + metadata  \n",
    "2. Load saved probabilities (preferred) or re-compute probabilities from the 2025 parquet  \n",
    "3. Run diagnostics (feature importance, confusion matrix, per-species metrics, calibration curves)  \n",
    "4. Export calibrated probabilities for downstream fusion  \n",
    "\n",
    "> Best practice: keep training and analysis separate so plots/calibration experiments do not accidentally change training artefacts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "Imports and basic utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    log_loss,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")\n",
    "\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"figure.dpi\"] = 120\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 2. Paths\n",
    "Repo-relative paths (no absolute `/Users/...` paths). Adjust if your layout differs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from digital_naturalist.paths import load_paths\n",
    "\n",
    "# Single source of truth for repo paths\n",
    "P = load_paths(\"configs/paths.yaml\")\n",
    "\n",
    "REPO_ROOT = P[\"REPO_ROOT\"]\n",
    "\n",
    "MODEL_DIR = P[\"CONTEXT_MODEL_DIR\"]\n",
    "OUT_DIR   = P[\"OUT_CONTEXT_XGB\"]\n",
    "\n",
    "FIG_DIR     = OUT_DIR / \"figures\"\n",
    "METRICS_DIR = OUT_DIR / \"metrics\"\n",
    "PREDS_DIR   = OUT_DIR / \"preds\"\n",
    "\n",
    "for d in (FIG_DIR, METRICS_DIR, PREDS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH       = MODEL_DIR / \"xgboost_hsi_model_FINAL_no_vespula.json\"\n",
    "FEATURES_PATH    = MODEL_DIR / \"feature_names_FINAL_no_vespula.csv\"\n",
    "SPECIES_MAP_PATH = MODEL_DIR / \"species_mapping_FINAL_no_vespula.csv\"\n",
    "\n",
    "# Saved probabilities (preferred)\n",
    "TEST_PREDS_PATH = PREDS_DIR / \"test_predictions_2025_FINAL_no_vespula.parquet\"\n",
    "\n",
    "# Optional (only if you saved them in training notebook)\n",
    "TRAIN_PREDS_PATH = PREDS_DIR / \"train_predictions_with_hsi.parquet\"\n",
    "VAL_PREDS_PATH   = PREDS_DIR / \"val_predictions_with_hsi.parquet\"\n",
    "\n",
    "# Raw parquets (only needed if you do feature-based or group-based recalibration)\n",
    "HISTORICAL_PATH = P[\"GBIF_TRAIN_DIR\"] / \"observations_filtered_50m_accuracy.parquet\"\n",
    "TEST_2025_PATH  = P[\"GBIF_VAL_DIR\"]   / \"observations_filtered_50m_accuracy.parquet\"\n",
    "\n",
    "print(\"=== Context analysis (XGBoost) — resolved paths ===\")\n",
    "print(\"Repo root:       \", REPO_ROOT)\n",
    "print(\"Model:           \", MODEL_PATH)\n",
    "print(\"Feature names:   \", FEATURES_PATH)\n",
    "print(\"Species mapping: \", SPECIES_MAP_PATH)\n",
    "print(\"Test preds:      \", TEST_PREDS_PATH)\n",
    "print(\"Train preds:     \", TRAIN_PREDS_PATH)\n",
    "print(\"Val preds:       \", VAL_PREDS_PATH)\n",
    "print(\"Historical raw:  \", HISTORICAL_PATH)\n",
    "print(\"2025 raw:        \", TEST_2025_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Load model + metadata\n",
    "Loads the trained XGBoost model, canonical feature order, and species index mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MODEL_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Model not found: {MODEL_PATH}\")\n",
    "if not FEATURES_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Feature list not found: {FEATURES_PATH}\")\n",
    "if not SPECIES_MAP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Species mapping not found: {SPECIES_MAP_PATH}\")\n",
    "\n",
    "# Load model\n",
    "model = xgb.XGBClassifier()\n",
    "model.load_model(MODEL_PATH)\n",
    "\n",
    "# Feature list (order matters!)\n",
    "feature_names = pd.read_csv(FEATURES_PATH)[\"feature\"].tolist()\n",
    "\n",
    "# Species mapping (order matters!)\n",
    "species_map = pd.read_csv(SPECIES_MAP_PATH).sort_values(\"idx\")\n",
    "species_names = species_map[\"species\"].tolist()\n",
    "idx_to_species = dict(zip(species_map[\"idx\"], species_map[\"species\"]))\n",
    "species_to_idx = dict(zip(species_map[\"species\"], species_map[\"idx\"]))\n",
    "n_classes = len(species_names)\n",
    "\n",
    "print(\"n_features:\", len(feature_names))\n",
    "print(\"n_classes:\", n_classes)\n",
    "print(\"species order:\", species_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 4. Load test labels and probabilities\n",
    "\n",
    "**Preferred:** load saved test-set probabilities produced by the training notebook.\n",
    "\n",
    "If those are not available, the fallback recomputes probabilities from the 2025 parquet using the saved feature list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_col_for_species(species: str) -> str:\n",
    "    return f\"prob_{species.replace(' ', '_')}\"\n",
    "\n",
    "def get_prob_cols(df: pd.DataFrame) -> list[str]:\n",
    "    cols = []\n",
    "    for sp in species_names:\n",
    "        c = prob_col_for_species(sp)\n",
    "        if c in df.columns:\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "test_df = None  # will be set if available\n",
    "test_preds = None\n",
    "\n",
    "# ---- Preferred: load saved predictions parquet ----\n",
    "if TEST_PREDS_PATH.exists():\n",
    "    test_preds = pd.read_parquet(TEST_PREDS_PATH)\n",
    "    prob_cols = get_prob_cols(test_preds)\n",
    "\n",
    "    if len(prob_cols) != n_classes:\n",
    "        raise ValueError(f\"Expected {n_classes} prob columns in {TEST_PREDS_PATH.name}, found {len(prob_cols)}.\")\n",
    "\n",
    "    if \"species\" not in test_preds.columns:\n",
    "        raise ValueError(\"Saved predictions parquet must contain a 'species' column.\")\n",
    "\n",
    "    y_test = test_preds[\"species\"].map(species_to_idx).to_numpy()\n",
    "    y_proba_test = test_preds[prob_cols].to_numpy()\n",
    "    y_pred_test = y_proba_test.argmax(axis=1)\n",
    "\n",
    "    print(f\"Loaded saved test probabilities: {len(test_preds):,} rows\")\n",
    "else:\n",
    "    print(\"Saved test predictions not found. Falling back to recomputing from 2025 parquet...\")\n",
    "\n",
    "    if not TEST_2025_PATH.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"2025 parquet not found: {TEST_2025_PATH}\\n\"\n",
    "            \"Either generate saved predictions from the training notebook, or make sure the 2025 parquet exists.\"\n",
    "        )\n",
    "\n",
    "    test_df = pd.read_parquet(TEST_2025_PATH).copy()\n",
    "    test_df = test_df[test_df[\"species\"].isin(species_names)].copy()\n",
    "\n",
    "    missing = [f for f in feature_names if f not in test_df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            \"Cannot recompute probabilities because engineered features are missing in the parquet.\\n\"\n",
    "            f\"Missing {len(missing)} features (showing up to 20): {missing[:20]}\\n\"\n",
    "            \"Tip: prefer loading the saved predictions parquet produced by the training notebook.\"\n",
    "        )\n",
    "\n",
    "    X_test = test_df[feature_names].fillna(0).to_numpy()\n",
    "    y_test = test_df[\"species\"].map(species_to_idx).to_numpy()\n",
    "\n",
    "    y_proba_test = model.predict_proba(X_test)\n",
    "    y_pred_test = y_proba_test.argmax(axis=1)\n",
    "\n",
    "    print(f\"Recomputed test probabilities: {len(test_df):,} rows\")\n",
    "\n",
    "# Sanity check\n",
    "print(\"y_proba_test shape:\", y_proba_test.shape)\n",
    "print(\"prob sum max abs error:\", float(np.max(np.abs(y_proba_test.sum(axis=1) - 1.0))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Metrics helpers\n",
    "Top-k accuracy and **normalised** multiclass Brier score (so values are comparable to your thesis scale ~0.10).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_accuracy(y_true: np.ndarray, y_proba: np.ndarray, k: int) -> float:\n",
    "    topk = np.argsort(y_proba, axis=1)[:, -k:]\n",
    "    return float(np.mean([y_true[i] in topk[i] for i in range(len(y_true))]))\n",
    "\n",
    "def multiclass_brier_norm(y_true: np.ndarray, y_proba: np.ndarray, n_classes: int) -> float:\n",
    "    # Normalised multiclass Brier: mean(sum_k (p_k - y_k)^2) / K\n",
    "    Y = np.eye(n_classes)[y_true]\n",
    "    return float(np.mean(np.sum((y_proba - Y) ** 2, axis=1)) / n_classes)\n",
    "\n",
    "def per_species_brier(y_true: np.ndarray, y_proba: np.ndarray, species_names: list[str]) -> pd.DataFrame:\n",
    "    # Binary Brier per class (OvR)\n",
    "    rows = []\n",
    "    for i, sp in enumerate(species_names):\n",
    "        y_bin = (y_true == i).astype(int)\n",
    "        p = y_proba[:, i]\n",
    "        rows.append({\"species\": sp, \"brier\": float(np.mean((p - y_bin) ** 2))})\n",
    "    return pd.DataFrame(rows).sort_values(\"brier\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 6. Baseline performance (uncalibrated)\n",
    "Classification report + log loss + Brier on the 2025 temporal holdout.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top-1:\", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Top-3:\", top_k_accuracy(y_test, y_proba_test, 3))\n",
    "print(\"Top-5:\", top_k_accuracy(y_test, y_proba_test, 5))\n",
    "print(\"Log loss:\", log_loss(y_test, y_proba_test, labels=list(range(n_classes))))\n",
    "print(\"Brier (norm):\", multiclass_brier_norm(y_test, y_proba_test, n_classes))\n",
    "\n",
    "print(\"\\nClassification report (test):\")\n",
    "print(classification_report(y_test, y_pred_test, target_names=species_names, zero_division=0))\n",
    "\n",
    "brier_df = per_species_brier(y_test, y_proba_test, species_names)\n",
    "brier_df.to_csv(METRICS_DIR / \"brier_per_species_uncalibrated_test.csv\", index=False)\n",
    "print(\"\\nSaved per-species Brier:\", METRICS_DIR / \"brier_per_species_uncalibrated_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes per-class One-vs-Rest AUC and mean predicted probability of the true class\n",
    "# Uses existing variables: y_test, y_proba_test, y_pred_test, species_names, n_classes, METRICS_DIR\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "# Get precision/recall/F1/support as a dict\n",
    "report_dict = classification_report(\n",
    "    y_test, y_pred_test,\n",
    "    target_names=species_names,\n",
    "    zero_division=0,\n",
    "    output_dict=True,\n",
    ")\n",
    "\n",
    "rows = []\n",
    "for i, sp in enumerate(species_names):\n",
    "    mask = (y_test == i)\n",
    "    y_bin = mask.astype(int)  # 1 for class i, 0 otherwise\n",
    "    p = y_proba_test[:, i]\n",
    "\n",
    "    # OvR AUC (undefined if only one class present)\n",
    "    if y_bin.sum() == 0 or y_bin.sum() == len(y_bin):\n",
    "        auc = float(\"nan\")\n",
    "    else:\n",
    "        auc = float(roc_auc_score(y_bin, p))\n",
    "\n",
    "    # Mean p(true class | x) for samples whose true label is this class\n",
    "    mean_p_true = float(np.mean(y_proba_test[mask, i])) if mask.any() else float(\"nan\")\n",
    "\n",
    "    # Binary Brier per class (OvR)\n",
    "    brier = float(np.mean((p - y_bin) ** 2))\n",
    "\n",
    "    rows.append({\n",
    "        \"species\": sp,\n",
    "        \"N\": int(report_dict[sp][\"support\"]),\n",
    "        \"precision\": float(report_dict[sp][\"precision\"]),\n",
    "        \"recall\": float(report_dict[sp][\"recall\"]),\n",
    "        \"f1\": float(report_dict[sp][\"f1-score\"]),\n",
    "        \"auc\": auc,\n",
    "        \"mean_p_true\": mean_p_true,\n",
    "        \"brier\": brier,\n",
    "    })\n",
    "\n",
    "per_species_metrics = pd.DataFrame(rows)\n",
    "per_species_metrics.to_csv(METRICS_DIR / \"per_species_metrics_uncalibrated_test.csv\", index=False)\n",
    "print(\"Saved:\", METRICS_DIR / \"per_species_metrics_uncalibrated_test.csv\")\n",
    "\n",
    "# Pretty view (rounded) for quick copying into LaTeX\n",
    "display(\n",
    "    per_species_metrics.assign(\n",
    "        precision=lambda d: d[\"precision\"].round(3),\n",
    "        recall=lambda d: d[\"recall\"].round(3),\n",
    "        f1=lambda d: d[\"f1\"].round(3),\n",
    "        auc=lambda d: d[\"auc\"].round(3),\n",
    "        mean_p_true=lambda d: d[\"mean_p_true\"].round(3),\n",
    "        brier=lambda d: d[\"brier\"].round(4),\n",
    "    )\n",
    ")\n",
    "\n",
    "# LaTeX helper: print rows with the same rounding as your thesis table\n",
    "print(\"\\nLaTeX rows (copy into your tabular):\\n\")\n",
    "for _, r in per_species_metrics.iterrows():\n",
    "    sp_tex = f\"\\\\textit{{{r['species']}}}\"\n",
    "    print(\n",
    "        f\"{sp_tex} & {int(r['N'])} \"\n",
    "        f\"& {r['precision']:.3f} & {r['recall']:.3f} & {r['f1']:.3f} \"\n",
    "        f\"& {r['auc']:.3f} & {r['mean_p_true']:.3f} & {r['brier']:.4f} \\\\\\\\\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Feature importance\n",
    "Uses the model’s built-in feature importance. This does not require any data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(model, feature_names, top_n=40, out_path=None) -> pd.DataFrame:\n",
    "    importance = model.feature_importances_\n",
    "    feat_imp_df = (\n",
    "        pd.DataFrame({\"feature\": feature_names, \"importance\": importance})\n",
    "        .sort_values(\"importance\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "    print(f\"Top {top_n} features:\")\n",
    "    for _, row in feat_imp_df.head(top_n).iterrows():\n",
    "        print(f\"{row['feature']:.<55} {row['importance']:>8.4f}\")\n",
    "\n",
    "    top = feat_imp_df.head(top_n).iloc[::-1]\n",
    "    fig, ax = plt.subplots(figsize=(10, max(6, 0.25 * top_n)))\n",
    "    ax.barh(top[\"feature\"], top[\"importance\"])\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    ax.set_title(f\"Top {top_n} feature importances\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if out_path is None:\n",
    "        out_path = FIG_DIR / \"feature_importance_top.png\"\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(\"Saved:\", out_path)\n",
    "\n",
    "    return feat_imp_df\n",
    "\n",
    "feat_imp_df = analyze_feature_importance(model, feature_names, top_n=40)\n",
    "feat_imp_df.to_csv(METRICS_DIR / \"feature_importance.csv\", index=False)\n",
    "print(\"Saved:\", METRICS_DIR / \"feature_importance.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 8. Confusion matrix\n",
    "Counts + row-normalised heatmap.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_test, labels=list(range(n_classes)))\n",
    "cm_norm = cm.astype(float) / np.maximum(cm.sum(axis=1, keepdims=True), 1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_norm,\n",
    "    annot=cm,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[s.replace(\" \", \"\\n\") for s in species_names],\n",
    "    yticklabels=[s.replace(\" \", \"\\n\") for s in species_names],\n",
    "    cbar_kws={\"label\": \"Row-normalized proportion\"},\n",
    "    ax=ax,\n",
    "    square=True\n",
    ")\n",
    "\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_title(\"Confusion matrix (counts annotated; colours row-normalised)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "out_path = FIG_DIR / \"confusion_matrix_test.png\"\n",
    "fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 9. Calibration curves (reliability diagrams)\n",
    "Per-species curves using quantile binning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_calibration_curves(y_true, y_proba, species_names, n_bins=10, out_path=None, title=None):\n",
    "    n_species = len(species_names)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_species + n_cols - 1) // n_cols\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4 * n_rows))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "\n",
    "    for i, sp in enumerate(species_names):\n",
    "        ax = axes[i]\n",
    "        y_bin = (y_true == i).astype(int)\n",
    "        p = y_proba[:, i]\n",
    "\n",
    "        prob_true, prob_pred = calibration_curve(y_bin, p, n_bins=n_bins, strategy=\"quantile\")\n",
    "        ax.plot(prob_pred, prob_true, marker=\"o\", linewidth=2)\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", alpha=0.5)\n",
    "\n",
    "        ax.set_xlim(0, 1)\n",
    "        ax.set_ylim(0, 1)\n",
    "        ax.grid(alpha=0.3)\n",
    "        ax.set_title(sp.replace(\" \", \"\\n\"), fontsize=9, fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Mean predicted probability\")\n",
    "        ax.set_ylabel(\"Fraction of positives\")\n",
    "\n",
    "    for j in range(n_species, len(axes)):\n",
    "        axes[j].axis(\"off\")\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, y=1.02)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if out_path is not None:\n",
    "        fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "        print(\"Saved:\", out_path)\n",
    "\n",
    "    return fig\n",
    "\n",
    "_ = plot_calibration_curves(\n",
    "    y_test, y_proba_test, species_names, n_bins=10,\n",
    "    out_path=FIG_DIR / \"calibration_curves_uncalibrated_test.png\",\n",
    "    title=\"Calibration curves (uncalibrated) — test\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 10. Notes for thesis reporting\n",
    "\n",
    "- Report calibration metrics on the **2025 temporal holdout** (test set).  \n",
    "- Use **normalised** multiclass Brier score for comparability across number of classes.  \n",
    "- If isotonic improves both log loss and Brier, prefer isotonic over sigmoid.  \n",
    "- Keep large artefacts (`data/`, `models/`, `outputs/`) gitignored in the public repo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis-py312)",
   "language": "python",
   "name": "thesis-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
