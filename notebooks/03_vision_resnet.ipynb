{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# ResNet‑18 training notebook\n",
    "\n",
    "This notebook trains and evaluates an image classifier (ResNet‑18) for Amsterdam insect species.\n",
    "\n",
    "**What this notebook does**\n",
    "- Loads an `ImageFolder` dataset with a **train / val / test** directory structure.\n",
    "- Trains **N independent runs** (different random seeds) with early stopping.\n",
    "- Evaluates each run on a **temporal holdout** test split (e.g. `test2` = year 2025).\n",
    "- Saves weights, per-run metrics, predictions, and confusion matrices.\n",
    "- Produces an aggregated summary across runs (mean ± std).\n",
    "\n",
    "**Repository-aligned outputs (your current structure)**\n",
    "\n",
    "- **Models** (weights): `models/vision/`\n",
    "- **Run artifacts** (figures / metrics / predictions):\n",
    "  - `outputs/vision_resnet/figures/`\n",
    "  - `outputs/vision_resnet/metrics/`\n",
    "  - `outputs/vision_resnet/preds/`\n",
    "\n",
    "**Recommended dataset folder structure**\n",
    "\n",
    "```\n",
    "data/amsterdam/images_no_vespula/\n",
    "  train/\n",
    "    Species_A/\n",
    "      img_001.jpg\n",
    "      ...\n",
    "  val/\n",
    "    Species_A/\n",
    "      ...\n",
    "  test2/           # temporal holdout (e.g. 2025)\n",
    "    Species_A/\n",
    "      ...\n",
    "```\n",
    "\n",
    "> This notebook is robust to being launched from `notebooks/` by auto-detecting the repo root using `pyproject.toml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment sanity check (helps reproducibility)\n",
    "import sys, platform\n",
    "import torch, torchvision\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "print(\"Torchvision:\", torchvision.__version__)\n",
    "print(\"MPS available:\", getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available())\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "\n",
    "Edit the paths and hyperparameters below.\n",
    "\n",
    "For portability, the defaults assume you run this notebook from the repository root, with data under `./data/...`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from digital_naturalist.paths import load_paths\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Load repo paths (single source of truth)\n",
    "# -----------------------------\n",
    "P = load_paths(\"configs/paths.yaml\")\n",
    "\n",
    "# Main dirs\n",
    "REPO_ROOT = P[\"REPO_ROOT\"]\n",
    "IMAGE_ROOT = P[\"IMAGE_ROOT\"]              # data/.../images_no_vespula\n",
    "IMAGE_TRAIN = P[\"IMAGE_TRAIN_DIR\"]\n",
    "IMAGE_VAL   = P[\"IMAGE_VAL_DIR\"]\n",
    "IMAGE_TEST2 = P[\"IMAGE_TEST2_DIR\"]\n",
    "\n",
    "MODELS_DIR = P[\"VISION_MODEL_DIR\"]        # models/vision\n",
    "TEMPS_DIR  = P[\"VISION_TEMPS_DIR\"]        # models/vision/temperatures\n",
    "\n",
    "OUT_DIR = P[\"OUT_VISION_RESNET\"]          # outputs/vision_resnet\n",
    "FIG_DIR = OUT_DIR / \"figures\"\n",
    "MET_DIR = OUT_DIR / \"metrics\"\n",
    "PRD_DIR = OUT_DIR / \"preds\"\n",
    "\n",
    "for d in (FIG_DIR, MET_DIR, PRD_DIR, MODELS_DIR, TEMPS_DIR):\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Experiment config (non-path hyperparams only)\n",
    "# -----------------------------\n",
    "@dataclass(frozen=True)\n",
    "class ExperimentConfig:\n",
    "    test_split_name: str = \"test2\"   # temporal holdout split\n",
    "    image_size: int = 224\n",
    "\n",
    "    num_classes: int = 8\n",
    "    batch_size: int = 32\n",
    "    lr: float = 1e-3\n",
    "    epochs: int = 20\n",
    "    patience: int = 4\n",
    "    num_runs: int = 10\n",
    "    base_seed: int = 42\n",
    "    num_workers: int = 0   # macOS/MPS safest\n",
    "    use_pretrained: bool = True\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if getattr(torch.backends, \"mps\", None) is not None and torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "CFG = ExperimentConfig()\n",
    "DEVICE = get_device()\n",
    "\n",
    "print(\"Repo root        :\", REPO_ROOT)\n",
    "print(\"Image root       :\", IMAGE_ROOT, \"| exists:\", IMAGE_ROOT.exists())\n",
    "print(\"Train images     :\", IMAGE_TRAIN, \"| exists:\", IMAGE_TRAIN.exists())\n",
    "print(\"Val images       :\", IMAGE_VAL, \"| exists:\", IMAGE_VAL.exists())\n",
    "print(\"Test images      :\", IMAGE_TEST2, \"| exists:\", IMAGE_TEST2.exists())\n",
    "print(\"Outputs root     :\", OUT_DIR)\n",
    "print(\"  figures        :\", FIG_DIR)\n",
    "print(\"  metrics        :\", MET_DIR)\n",
    "print(\"  preds          :\", PRD_DIR)\n",
    "print(\"Vision models    :\", MODELS_DIR)\n",
    "print(\"Temperatures     :\", TEMPS_DIR)\n",
    "print(\"Device           :\", DEVICE)\n",
    "\n",
    "# Save config snapshot (hyperparams + resolved paths)\n",
    "config_snapshot = {\n",
    "    \"paths\": {k: str(v) for k, v in P.items()},\n",
    "    \"cfg\": asdict(CFG),\n",
    "    \"device\": str(DEVICE),\n",
    "}\n",
    "with open(MET_DIR / \"config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(config_snapshot, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aliases \n",
    "TRAIN_DIR = IMAGE_TRAIN\n",
    "VAL_DIR   = IMAGE_VAL\n",
    "TEST_DIR  = IMAGE_TEST2  # your temporal holdout split\n",
    "\n",
    "# Sanity checks\n",
    "assert TRAIN_DIR.exists(), f\"Missing TRAIN_DIR: {TRAIN_DIR}\"\n",
    "assert VAL_DIR.exists(),   f\"Missing VAL_DIR: {VAL_DIR}\"\n",
    "assert TEST_DIR.exists(),  f\"Missing TEST_DIR: {TEST_DIR}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Where files will be saved (quick reference)\n",
    "\n",
    "- **Weights** → `models/vision/`\n",
    "- **Per-run predictions** → `outputs/vision_resnet/preds/<run_tag>/`\n",
    "- **Per-run metrics** → `outputs/vision_resnet/metrics/<run_tag>/`\n",
    "- **Per-run figures** → `outputs/vision_resnet/figures/<run_tag>/`\n",
    "- **Aggregates** → `outputs/vision_resnet/metrics/` and `outputs/vision_resnet/figures/`\n",
    "\n",
    "You can override folders with environment variables:\n",
    "\n",
    "- `DATA_DIR`\n",
    "- `OUTPUTS_DIR`\n",
    "- `MODELS_DIR`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2. Data loading\n",
    "\n",
    "We use `torchvision.datasets.ImageFolder`. This requires each split folder to contain one subfolder per class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "def _assert_split_exists(split_dir: Path) -> None:\n",
    "    if not split_dir.exists():\n",
    "        raise FileNotFoundError(f\"Missing split folder: {split_dir}\")\n",
    "    class_dirs = [p for p in split_dir.iterdir() if p.is_dir()]\n",
    "    if not class_dirs:\n",
    "        raise RuntimeError(f\"No class subfolders found in: {split_dir}\")\n",
    "\n",
    "\n",
    "def build_transforms(image_size: int) -> Dict[str, transforms.Compose]:\n",
    "    '''Standard ImageNet normalization with mild augmentation for training.'''\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    return {\n",
    "        \"train\": transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        ]),\n",
    "        \"val\": transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        ]),\n",
    "        \"test\": transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "\n",
    "def build_datasets(cfg: ExperimentConfig) -> Tuple[Dict[str, datasets.ImageFolder], List[str]]:\n",
    "    split_train = IMAGE_ROOT / \"train\"\n",
    "    split_val = IMAGE_ROOT / \"val\"\n",
    "    split_test = IMAGE_ROOT / cfg.test_split_name\n",
    "\n",
    "    _assert_split_exists(split_train)\n",
    "    _assert_split_exists(split_val)\n",
    "    _assert_split_exists(split_test)\n",
    "\n",
    "    tfms = build_transforms(cfg.image_size)\n",
    "\n",
    "    ds = {\n",
    "        \"train\": datasets.ImageFolder(split_train, transform=tfms[\"train\"]),\n",
    "        \"val\": datasets.ImageFolder(split_val, transform=tfms[\"val\"]),\n",
    "        \"test\": datasets.ImageFolder(split_test, transform=tfms[\"test\"]),\n",
    "    }\n",
    "\n",
    "    classes = ds[\"train\"].classes\n",
    "    for k in (\"val\", \"test\"):\n",
    "        if ds[k].classes != classes:\n",
    "            raise RuntimeError(\n",
    "                f\"Class mismatch between train and {k}. \"\n",
    "                f\"train={classes}, {k}={ds[k].classes}\"\n",
    "            )\n",
    "\n",
    "    return ds, classes\n",
    "\n",
    "\n",
    "image_datasets, class_names = build_datasets(CFG)\n",
    "\n",
    "print(f\"Train: {len(image_datasets['train'])} images\")\n",
    "print(f\"Val:   {len(image_datasets['val'])} images\")\n",
    "print(f\"Test:  {len(image_datasets['test'])} images (split='{CFG.test_split_name}')\")\n",
    "print(\"Classes:\", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "ResNet‑18 with a replaced final fully‑connected layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "def create_resnet18(num_classes: int, pretrained: bool = True) -> nn.Module:\n",
    "    '''\n",
    "    Create ResNet-18 with a custom classification head.\n",
    "\n",
    "    Uses the newer torchvision `Weights` API when available, with a fallback.\n",
    "    '''\n",
    "    if pretrained:\n",
    "        try:\n",
    "            weights = models.ResNet18_Weights.IMAGENET1K_V1  # torchvision >= 0.13\n",
    "            model = models.resnet18(weights=weights)\n",
    "        except Exception:\n",
    "            model = models.resnet18(pretrained=True)  # older torchvision\n",
    "    else:\n",
    "        try:\n",
    "            model = models.resnet18(weights=None)\n",
    "        except Exception:\n",
    "            model = models.resnet18(pretrained=False)\n",
    "\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 4. Training & evaluation utilities\n",
    "\n",
    "- Early stopping on **validation loss**\n",
    "- Accuracy + macro precision/recall/F1 on the test set\n",
    "- Top‑K accuracy (K ∈ {1,3,5})\n",
    "- Confusion matrix saved per run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import copy\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "\n",
    "def set_seed(seed: int) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "def make_dataloaders(cfg: ExperimentConfig, datasets_dict) -> Dict[str, DataLoader]:\n",
    "    pin = DEVICE.type == \"cuda\"  # pin_memory only helps CUDA\n",
    "    return {\n",
    "        split: DataLoader(\n",
    "            datasets_dict[split],\n",
    "            batch_size=cfg.batch_size,\n",
    "            shuffle=(split == \"train\"),\n",
    "            num_workers=cfg.num_workers,\n",
    "            pin_memory=pin,\n",
    "        )\n",
    "        for split in (\"train\", \"val\", \"test\")\n",
    "    }\n",
    "\n",
    "\n",
    "def topk_accuracy(y_true: np.ndarray, probs: np.ndarray, k: int) -> float:\n",
    "    topk = np.argsort(probs, axis=1)[:, -k:]\n",
    "    hits = [yt in row for yt, row in zip(y_true, topk)]\n",
    "    return float(np.mean(hits))\n",
    "\n",
    "\n",
    "def train_one_run(\n",
    "    model: nn.Module,\n",
    "    dataloaders: Dict[str, DataLoader],\n",
    "    cfg: ExperimentConfig,\n",
    ") -> Tuple[nn.Module, float, float]:\n",
    "    '''\n",
    "    Trains a model with early stopping on validation loss.\n",
    "    Returns (best_model, best_val_loss, best_val_acc).\n",
    "    '''\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "    best_wts = copy.deepcopy(model.state_dict())\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        for phase in (\"train\", \"val\"):\n",
    "            is_train = phase == \"train\"\n",
    "            model.train(is_train)\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            n = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "\n",
    "                    if is_train:\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                bs = inputs.size(0)\n",
    "                running_loss += loss.item() * bs\n",
    "                running_corrects += (preds == labels).sum().item()\n",
    "                n += bs\n",
    "\n",
    "            epoch_loss = running_loss / max(n, 1)\n",
    "            epoch_acc = running_corrects / max(n, 1)\n",
    "\n",
    "            if is_train:\n",
    "                scheduler.step()\n",
    "\n",
    "            if phase == \"val\":\n",
    "                if epoch_loss < best_val_loss - 1e-6:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_val_acc = epoch_acc\n",
    "                    best_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= cfg.patience:\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_wts)\n",
    "    return model, float(best_val_loss), float(best_val_acc)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, dataloader: DataLoader, num_classes: int) -> Dict[str, Any]:\n",
    "    model.eval()\n",
    "    all_preds: list[int] = []\n",
    "    all_labels: list[int] = []\n",
    "    all_probs: list[np.ndarray] = []\n",
    "\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.softmax(outputs, dim=1).cpu().numpy()\n",
    "        preds = outputs.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "        all_probs.append(probs)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(labels.numpy().tolist())\n",
    "\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    probs = np.vstack(all_probs) if len(all_probs) else np.zeros((0, num_classes), dtype=float)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred) if len(y_true) else float(\"nan\")\n",
    "    prec = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes)))\n",
    "\n",
    "    return {\n",
    "        \"y_true\": y_true,\n",
    "        \"y_pred\": y_pred,\n",
    "        \"probs\": probs,\n",
    "        \"accuracy\": float(acc),\n",
    "        \"precision_macro\": float(prec),\n",
    "        \"recall_macro\": float(rec),\n",
    "        \"f1_macro\": float(f1),\n",
    "        \"top1\": topk_accuracy(y_true, probs, 1) if len(y_true) else float(\"nan\"),\n",
    "        \"top3\": topk_accuracy(y_true, probs, 3) if len(y_true) else float(\"nan\"),\n",
    "        \"top5\": topk_accuracy(y_true, probs, 5) if len(y_true) else float(\"nan\"),\n",
    "        \"cm\": cm,\n",
    "        \"report\": classification_report(y_true, y_pred, output_dict=True, zero_division=0),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 5. Plotting & saving artifacts\n",
    "\n",
    "The helpers below save run outputs to `OUTPUT_DIR/run_XX_seed_YY/...`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def _pretty_class_name(name: str) -> str:\n",
    "    return name.replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "def save_confusion_matrix_png(\n",
    "    cm: np.ndarray,\n",
    "    class_names: List[str],\n",
    "    out_path: Path,\n",
    "    normalize: bool = True,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save a confusion matrix PNG (no seaborn).\n",
    "    - normalize=False: shows counts (int if possible; otherwise 1 decimal for mean CMs)\n",
    "    - normalize=True: shows row-normalized percentages\n",
    "    \"\"\"\n",
    "    cm = np.asarray(cm)\n",
    "\n",
    "    if normalize:\n",
    "        cm_plot = cm.astype(float)\n",
    "        row_sums = cm_plot.sum(axis=1, keepdims=True)\n",
    "        cm_plot = np.divide(cm_plot, np.maximum(row_sums, 1e-12))\n",
    "        fmt = \".1%\"  # show as percentage\n",
    "    else:\n",
    "        # If counts are integer-like (typical per-run CM), display as ints.\n",
    "        # If not (e.g. mean across runs), display with 1 decimal.\n",
    "        is_intlike = np.all(np.isclose(cm, np.round(cm)))\n",
    "        if is_intlike:\n",
    "            cm_plot = np.round(cm).astype(int)\n",
    "            fmt = \"d\"\n",
    "        else:\n",
    "            cm_plot = cm.astype(float)\n",
    "            fmt = \".1f\"\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    im = ax.imshow(cm_plot, interpolation=\"nearest\")\n",
    "    fig.colorbar(im, ax=ax)\n",
    "\n",
    "    tick_labels = [_pretty_class_name(c) for c in class_names]\n",
    "    ax.set_xticks(np.arange(len(class_names)), labels=tick_labels, rotation=45, ha=\"right\")\n",
    "    ax.set_yticks(np.arange(len(class_names)), labels=tick_labels)\n",
    "\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_title(\"Confusion Matrix (normalized)\" if normalize else \"Confusion Matrix (counts)\")\n",
    "\n",
    "    thresh = (cm_plot.max() / 2.0) if cm_plot.size else 0.0\n",
    "    for i in range(cm_plot.shape[0]):\n",
    "        for j in range(cm_plot.shape[1]):\n",
    "            val = cm_plot[i, j]\n",
    "            text = f\"{val:{fmt}}\"\n",
    "            ax.text(\n",
    "                j, i, text,\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if val > thresh else \"black\",\n",
    "                fontsize=9,\n",
    "            )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    fig.savefig(out_path, dpi=300, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_run_artifacts(\n",
    "    run_tag: str,\n",
    "    class_names: List[str],\n",
    "    eval_dict: Dict[str, Any],\n",
    "    *,\n",
    "    fig_root: Path,\n",
    "    met_root: Path,\n",
    "    pred_root: Path,\n",
    ") -> None:\n",
    "    \"\"\"Save per-run artifacts aligned to outputs/vision_resnet/{figures,metrics,preds}.\"\"\"\n",
    "    run_fig = fig_root / run_tag\n",
    "    run_met = met_root / run_tag\n",
    "    run_prd = pred_root / run_tag\n",
    "    for d in (run_fig, run_met, run_prd):\n",
    "        d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- Metrics (JSON + small CSV) ----\n",
    "    metrics = {\n",
    "        \"accuracy\": float(eval_dict[\"accuracy\"]),\n",
    "        \"precision_macro\": float(eval_dict[\"precision_macro\"]),\n",
    "        \"recall_macro\": float(eval_dict[\"recall_macro\"]),\n",
    "        \"f1_macro\": float(eval_dict[\"f1_macro\"]),\n",
    "        \"top1\": float(eval_dict[\"top1\"]),\n",
    "        \"top3\": float(eval_dict[\"top3\"]),\n",
    "        \"top5\": float(eval_dict[\"top5\"]),\n",
    "    }\n",
    "    with open(run_met / f\"metrics_{run_tag}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    pd.DataFrame([{\"run\": run_tag, **metrics}]).to_csv(run_met / f\"metrics_{run_tag}.csv\", index=False)\n",
    "\n",
    "    # Full classification report as JSON (handy for thesis tables)\n",
    "    with open(run_met / f\"classification_report_{run_tag}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(eval_dict[\"report\"], f, indent=2)\n",
    "\n",
    "    # Per-class metrics CSV (precision/recall/f1/support)\n",
    "    report = eval_dict[\"report\"]\n",
    "    per_class = []\n",
    "    for cls in class_names:\n",
    "        if cls in report:\n",
    "            per_class.append({\n",
    "                \"class\": cls,\n",
    "                \"precision\": float(report[cls][\"precision\"]),\n",
    "                \"recall\": float(report[cls][\"recall\"]),\n",
    "                \"f1\": float(report[cls][\"f1-score\"]),\n",
    "                \"support\": int(report[cls][\"support\"]),\n",
    "            })\n",
    "    pd.DataFrame(per_class).to_csv(run_met / f\"per_class_metrics_{run_tag}.csv\", index=False)\n",
    "\n",
    "    # ---- Predictions ----\n",
    "    y_true = np.asarray(eval_dict[\"y_true\"], dtype=int)\n",
    "    y_pred = np.asarray(eval_dict[\"y_pred\"], dtype=int)\n",
    "    probs = np.asarray(eval_dict[\"probs\"], dtype=float)\n",
    "\n",
    "    pred_df = pd.DataFrame({\n",
    "        \"true_label\": y_true,\n",
    "        \"true_class\": [class_names[i] for i in y_true],\n",
    "        \"pred_label\": y_pred,\n",
    "        \"pred_class\": [class_names[i] for i in y_pred],\n",
    "        \"correct\": (y_true == y_pred),\n",
    "    })\n",
    "    for i, cls in enumerate(class_names):\n",
    "        pred_df[f\"prob_{cls}\"] = probs[:, i]\n",
    "    pred_df.to_csv(run_prd / f\"predictions_{run_tag}.csv\", index=False)\n",
    "\n",
    "    # ---- Confusion matrix (array + figures) ----\n",
    "    cm = np.asarray(eval_dict[\"cm\"])\n",
    "    np.save(run_met / f\"confusion_matrix_{run_tag}.npy\", cm)\n",
    "\n",
    "    save_confusion_matrix_png(cm, class_names, run_fig / f\"confusion_matrix_{run_tag}_counts.png\", normalize=False)\n",
    "    save_confusion_matrix_png(cm, class_names, run_fig / f\"confusion_matrix_{run_tag}_normalized.png\", normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 6. Main experiment: train **10 independent** ResNet‑18 models\n",
    "\n",
    "This is the *primary* training block (ported from the last block of your original notebook, but cleaned and modularised).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 6.1 Post‑hoc temperature scaling (per run)\n",
    "\n",
    "For each trained ResNet‑18 model, we learn a single scalar temperature **T\\*** on the **validation** split by minimising\n",
    "negative log-likelihood (cross‑entropy) on fixed logits (Guo et al., 2017).\n",
    "\n",
    "**Where outputs go**\n",
    "- Model weights: `models/vision/<run_tag>.pth`\n",
    "- Temperatures: `models/vision/temperatures/temperature_<run_tag>.npy`\n",
    "\n",
    "The learned T\\* is used later in fusion to calibrate the CNN probabilities without changing the predicted class (argmax is invariant under positive temperature scaling).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "\n",
    "def multiclass_brier_score(y_true: np.ndarray, y_proba: np.ndarray) -> float:\n",
    "    '''Vectorised multi-class Brier score (lower is better).'''\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_proba = np.asarray(y_proba, dtype=float)\n",
    "    n, k = y_proba.shape\n",
    "    y_onehot = np.zeros((n, k), dtype=float)\n",
    "    y_onehot[np.arange(n), y_true] = 1.0\n",
    "    return float(np.mean((y_proba - y_onehot) ** 2))\n",
    "\n",
    "\n",
    "def expected_calibration_error(y_true: np.ndarray, y_proba: np.ndarray, n_bins: int = 15) -> float:\n",
    "    '''Standard ECE (confidence vs. accuracy in bins).'''\n",
    "    y_true = np.asarray(y_true, dtype=int)\n",
    "    y_proba = np.asarray(y_proba, dtype=float)\n",
    "\n",
    "    confidences = y_proba.max(axis=1)\n",
    "    preds = y_proba.argmax(axis=1)\n",
    "    accuracies = (preds == y_true).astype(float)\n",
    "\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i + 1]\n",
    "        in_bin = (confidences > lo) & (confidences <= hi)\n",
    "        prop = in_bin.mean()\n",
    "        if prop > 0:\n",
    "            bin_acc = accuracies[in_bin].mean()\n",
    "            bin_conf = confidences[in_bin].mean()\n",
    "            ece += prop * abs(bin_acc - bin_conf)\n",
    "    return float(ece)\n",
    "\n",
    "\n",
    "class TemperatureScaler(nn.Module):\n",
    "    '''Single-parameter temperature scaling: logits / T, with T = exp(log_T) > 0.'''\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.log_T = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, logits: torch.Tensor) -> torch.Tensor:\n",
    "        T = torch.exp(self.log_T)\n",
    "        return logits / T\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits_and_labels_cpu(\n",
    "    model: nn.Module, loader: torch.utils.data.DataLoader, device: torch.device\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    '''\n",
    "    Collect logits and labels for a loader, returning CPU tensors.\n",
    "\n",
    "    We optimise temperature on CPU for maximum compatibility (LBFGS can be finicky on MPS).\n",
    "    '''\n",
    "    model.eval()\n",
    "    logits_list = []\n",
    "    labels_list = []\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        logits = model(inputs).detach().cpu()\n",
    "        logits_list.append(logits)\n",
    "        labels_list.append(labels.detach().cpu())\n",
    "    return torch.cat(logits_list, dim=0), torch.cat(labels_list, dim=0)\n",
    "\n",
    "\n",
    "def calibrate_temperature_from_val_logits(\n",
    "    logits_cpu: torch.Tensor, labels_cpu: torch.Tensor, max_iter: int = 50\n",
    ") -> float:\n",
    "    '''\n",
    "    Learn T* by minimising cross-entropy on validation logits (CPU).\n",
    "    Returns scalar T* (>0).\n",
    "    '''\n",
    "    temperature = TemperatureScaler().cpu()\n",
    "    nll = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.LBFGS(\n",
    "        temperature.parameters(),\n",
    "        lr=0.01,\n",
    "        max_iter=max_iter,\n",
    "        line_search_fn=\"strong_wolfe\",\n",
    "    )\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = nll(temperature(logits_cpu), labels_cpu)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    return float(torch.exp(temperature.log_T).item())\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_calibration(\n",
    "    model: nn.Module,\n",
    "    loader: torch.utils.data.DataLoader,\n",
    "    device: torch.device,\n",
    "    T_star: float | None = None,\n",
    ") -> Dict[str, float]:\n",
    "    '''Compute Brier + ECE, optionally applying temperature scaling.'''\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.to(device)\n",
    "        logits = model(inputs)\n",
    "        if T_star is not None:\n",
    "            logits = logits / float(T_star)\n",
    "        probs = F.softmax(logits, dim=1).detach().cpu().numpy()\n",
    "        probs_list.append(probs)\n",
    "        labels_list.append(labels.numpy())\n",
    "\n",
    "    probs = np.vstack(probs_list)\n",
    "    y_true = np.concatenate(labels_list)\n",
    "\n",
    "    return {\n",
    "        \"brier\": multiclass_brier_score(y_true, probs),\n",
    "        \"ece\": expected_calibration_error(y_true, probs, n_bins=15),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "results_rows = []\n",
    "cms = []\n",
    "\n",
    "for run in range(1, CFG.num_runs + 1):\n",
    "    seed = CFG.base_seed + (run - 1)\n",
    "    set_seed(seed)\n",
    "\n",
    "    run_tag = f\"resnet18_run_{run:02d}_seed_{seed}\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"RUN {run}/{CFG.num_runs}  |  seed={seed}  |  tag={run_tag}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    dataloaders = make_dataloaders(CFG, image_datasets)\n",
    "\n",
    "    model = create_resnet18(num_classes=CFG.num_classes, pretrained=CFG.use_pretrained).to(DEVICE)\n",
    "    model, best_val_loss, best_val_acc = train_one_run(model, dataloaders, CFG)\n",
    "\n",
    "    # Save weights to models/vision/\n",
    "    weights_path = MODELS_DIR / f\"{run_tag}.pth\"\n",
    "    torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "    # Evaluate on temporal holdout\n",
    "\n",
    "    # --- Temperature scaling (learn T* on val; save to models/vision/temperatures) ---\n",
    "    val_logits_cpu, val_labels_cpu = collect_logits_and_labels_cpu(model, dataloaders[\"val\"], DEVICE)\n",
    "    T_star = calibrate_temperature_from_val_logits(val_logits_cpu, val_labels_cpu, max_iter=50)\n",
    "    temperature_path = TEMPS_DIR/ f\"temperature_{run_tag}.npy\"\n",
    "    np.save(temperature_path, np.array([T_star], dtype=np.float32))\n",
    "\n",
    "    # Optional calibration metrics (does not change argmax / accuracy)\n",
    "    calib_before = evaluate_calibration(model, dataloaders[\"test\"], DEVICE, T_star=None)\n",
    "    calib_after = evaluate_calibration(model, dataloaders[\"test\"], DEVICE, T_star=T_star)\n",
    "\n",
    "    eval_dict = evaluate(model, dataloaders[\"test\"], num_classes=CFG.num_classes)\n",
    "\n",
    "    # Save artifacts to outputs/vision_resnet/{figures,metrics,preds}/\n",
    "    save_run_artifacts(\n",
    "        run_tag,\n",
    "        class_names,\n",
    "        eval_dict,\n",
    "        fig_root=FIG_DIR,\n",
    "        met_root=MET_DIR,\n",
    "        pred_root=PRD_DIR,\n",
    "    )\n",
    "\n",
    "    row = {\n",
    "        \"run\": run,\n",
    "        \"seed\": seed,\n",
    "        \"run_tag\": run_tag,\n",
    "        \"best_val_loss\": float(best_val_loss),\n",
    "        \"best_val_acc\": float(best_val_acc),\n",
    "        \"test_accuracy\": float(eval_dict[\"accuracy\"]),\n",
    "        \"test_precision_macro\": float(eval_dict[\"precision_macro\"]),\n",
    "        \"test_recall_macro\": float(eval_dict[\"recall_macro\"]),\n",
    "        \"test_f1_macro\": float(eval_dict[\"f1_macro\"]),\n",
    "        \"test_top1\": float(eval_dict[\"top1\"]),\n",
    "        \"test_top3\": float(eval_dict[\"top3\"]),\n",
    "        \"test_top5\": float(eval_dict[\"top5\"]),\n",
    "        \"weights_path\": str(weights_path),\n",
    "    }\n",
    "    row.update({\n",
    "        \"temperature_T\": float(T_star),\n",
    "        \"temperature_path\": str(temperature_path),\n",
    "        \"calib_brier_before\": float(calib_before[\"brier\"]),\n",
    "        \"calib_ece_before\": float(calib_before[\"ece\"]),\n",
    "        \"calib_brier_after\": float(calib_after[\"brier\"]),\n",
    "        \"calib_ece_after\": float(calib_after[\"ece\"]),\n",
    "    })\n",
    "\n",
    "    print(f\"Temp scaling: T*={T_star:.4f} | ECE {calib_before['ece']:.3f} -> {calib_after['ece']:.3f}\")\n",
    "\n",
    "    results_rows.append(row)\n",
    "    cms.append(eval_dict[\"cm\"])\n",
    "\n",
    "    print(\n",
    "        f\"Val acc={best_val_acc:.3f} | \"\n",
    "        f\"Test acc={eval_dict['accuracy']:.3f} | \"\n",
    "        f\"Top-3={eval_dict['top3']:.3f} | \"\n",
    "        f\"F1={eval_dict['f1_macro']:.3f}\"\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Aggregate summary (metrics)\n",
    "# -------------------------\n",
    "summary_df = pd.DataFrame(results_rows)\n",
    "\n",
    "# Match your existing naming convention (keep both for convenience)\n",
    "summary_df.to_csv(MET_DIR / \"summary_runs.csv\", index=False)\n",
    "summary_df.to_csv(MET_DIR / \"resnet18_10models_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"AGGREGATE RESULTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "def _mean_std(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    return float(np.nanmean(x)), float(np.nanstd(x))\n",
    "\n",
    "for col in [\"best_val_acc\", \"test_accuracy\", \"test_top3\", \"test_top5\", \"test_f1_macro\"]:\n",
    "    m, s = _mean_std(summary_df[col].values)\n",
    "    print(f\"{col:>15}: {m:.3f} ± {s:.3f}\")\n",
    "\n",
    "# Mean confusion matrix across runs\n",
    "mean_cm = np.mean(np.stack(cms, axis=0), axis=0)\n",
    "np.save(MET_DIR / \"confusion_matrix_resnet18_MEAN_10runs.npy\", mean_cm)\n",
    "\n",
    "save_confusion_matrix_png(mean_cm, class_names, FIG_DIR / \"confusion_matrix_resnet18_MEAN_10runs_counts.png\", normalize=False)\n",
    "save_confusion_matrix_png(mean_cm, class_names, FIG_DIR / \"confusion_matrix_resnet18_MEAN_10runs_normalized.png\", normalize=True)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\"-\", MET_DIR / \"summary_runs.csv\")\n",
    "print(\"-\", MET_DIR / \"resnet18_10models_summary.csv\")\n",
    "print(\"-\", FIG_DIR / \"confusion_matrix_resnet18_MEAN_10runs_counts.png\")\n",
    "print(\"-\", FIG_DIR / \"confusion_matrix_resnet18_MEAN_10runs_normalized.png\")\n",
    "print(\"-\", MET_DIR / \"confusion_matrix_resnet18_MEAN_10runs.npy\")\n",
    "print(\"-\", MODELS_DIR, \"(weights)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If you want, the next step would be to move the utilities into a small `src/` package (e.g. `src/data.py`, `src/train.py`) and keep the notebook as a thin “experiment driver”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Appendix A: dataset utilities (optional)\n",
    "\n",
    "The original notebook contained several one‑off scripts (splitting folders, resizing images, aligning parquet splits).\n",
    "These are useful, but they should not run by default during training.\n",
    "\n",
    "Set `RUN_UTILS = True` to execute any of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "RUN_UTILS = False  # <-- change to True if you want to execute these helpers\n",
    "\n",
    "\n",
    "def split_imagefolder_train_val(\n",
    "    train_dir: Path,\n",
    "    out_train_dir: Path,\n",
    "    out_val_dir: Path,\n",
    "    val_split: float = 0.2,\n",
    "    seed: int = 42,\n",
    "    copy_files: bool = True,\n",
    "    exts: Tuple[str, ...] = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\", \".tif\", \".tiff\"),\n",
    ") -> None:\n",
    "    '''\n",
    "    Create a deterministic train/val split from an ImageFolder-style `train_dir`.\n",
    "\n",
    "    Notes\n",
    "    - We write into new directories (no in-place overwrite).\n",
    "    - `copy_files=False` will MOVE files (faster, but destructive).\n",
    "    '''\n",
    "    rng = random.Random(seed)\n",
    "\n",
    "    train_dir = Path(train_dir)\n",
    "    out_train_dir = Path(out_train_dir)\n",
    "    out_val_dir = Path(out_val_dir)\n",
    "\n",
    "    if not train_dir.exists():\n",
    "        raise FileNotFoundError(train_dir)\n",
    "\n",
    "    out_train_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_val_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for cls_dir in sorted([p for p in train_dir.iterdir() if p.is_dir()]):\n",
    "        imgs = [p for p in cls_dir.iterdir() if p.suffix.lower() in exts]\n",
    "        rng.shuffle(imgs)\n",
    "\n",
    "        n_val = int(round(len(imgs) * val_split))\n",
    "        val_imgs = imgs[:n_val]\n",
    "        train_imgs = imgs[n_val:]\n",
    "\n",
    "        (out_train_dir / cls_dir.name).mkdir(exist_ok=True)\n",
    "        (out_val_dir / cls_dir.name).mkdir(exist_ok=True)\n",
    "\n",
    "        for p in train_imgs:\n",
    "            dst = out_train_dir / cls_dir.name / p.name\n",
    "            (shutil.copy2 if copy_files else shutil.move)(p, dst)\n",
    "\n",
    "        for p in val_imgs:\n",
    "            dst = out_val_dir / cls_dir.name / p.name\n",
    "            (shutil.copy2 if copy_files else shutil.move)(p, dst)\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(\"Train split:\", out_train_dir)\n",
    "    print(\"Val split  :\", out_val_dir)\n",
    "\n",
    "\n",
    "if RUN_UTILS:\n",
    "    # Example (adjust paths):\n",
    "    # split_imagefolder_train_val(\n",
    "    #     train_dir=CFG.data_dir / \"train\",\n",
    "    #     out_train_dir=CFG.data_dir / \"train_split\",\n",
    "    #     out_val_dir=CFG.data_dir / \"val\",\n",
    "    #     val_split=0.2,\n",
    "    #     seed=42,\n",
    "    #     copy_files=True,\n",
    "    # )\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Appendix B: align parquet splits with image splits (optional)\n",
    "\n",
    "If you maintain a metadata table (e.g. `train_with_image_paths.parquet`) and want it split **exactly** like your image folders,\n",
    "this helper will create `train.parquet` and `val.parquet` based on which image filenames appear in each split folder.\n",
    "\n",
    "This refactors the original “split parquet file to match image split” block into a reusable function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Set, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def collect_filenames(split_dir: Path) -> Set[str]:\n",
    "    '''Collect all filenames in an ImageFolder-style split (split/class_name/*.jpg).'''\n",
    "    split_dir = Path(split_dir)\n",
    "    names: Set[str] = set()\n",
    "    for cls_dir in split_dir.iterdir():\n",
    "        if not cls_dir.is_dir():\n",
    "            continue\n",
    "        for p in cls_dir.iterdir():\n",
    "            if p.is_file():\n",
    "                names.add(p.name)\n",
    "    return names\n",
    "\n",
    "\n",
    "def split_parquet_by_image_splits(\n",
    "    parquet_path: Path,\n",
    "    image_path_col: str,\n",
    "    train_split_dir: Path,\n",
    "    val_split_dir: Path,\n",
    "    out_train_path: Path,\n",
    "    out_val_path: Path,\n",
    ") -> Tuple[Path, Path]:\n",
    "    '''\n",
    "    Split a parquet table into train/val tables by matching image filenames.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    parquet_path : Path\n",
    "        Input parquet file (contains a column with image paths).\n",
    "    image_path_col : str\n",
    "        Column name holding image paths.\n",
    "    train_split_dir / val_split_dir : Path\n",
    "        Folders containing the split images.\n",
    "    out_train_path / out_val_path : Path\n",
    "        Output parquet paths.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (out_train_path, out_val_path)\n",
    "    '''\n",
    "    parquet_path = Path(parquet_path)\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if image_path_col not in df.columns:\n",
    "        raise KeyError(f\"'{image_path_col}' not found in parquet columns: {list(df.columns)[:10]} ...\")\n",
    "\n",
    "    train_files = collect_filenames(train_split_dir)\n",
    "    val_files = collect_filenames(val_split_dir)\n",
    "\n",
    "    df = df.copy()\n",
    "    df[\"_image_filename\"] = df[image_path_col].apply(lambda x: Path(x).name)\n",
    "\n",
    "    train_df = df[df[\"_image_filename\"].isin(train_files)].drop(columns=[\"_image_filename\"])\n",
    "    val_df = df[df[\"_image_filename\"].isin(val_files)].drop(columns=[\"_image_filename\"])\n",
    "\n",
    "    out_train_path = Path(out_train_path)\n",
    "    out_val_path = Path(out_val_path)\n",
    "    out_train_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    out_val_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_df.to_parquet(out_train_path)\n",
    "    val_df.to_parquet(out_val_path)\n",
    "\n",
    "    print(\"Saved:\")\n",
    "    print(\"-\", out_train_path, f\"({len(train_df)} rows)\")\n",
    "    print(\"-\", out_val_path, f\"({len(val_df)} rows)\")\n",
    "\n",
    "    return out_train_path, out_val_path\n",
    "\n",
    "\n",
    "# Example (disabled by default):\n",
    "# RUN_UTILS = True\n",
    "# if RUN_UTILS:\n",
    "#     split_parquet_by_image_splits(\n",
    "#         parquet_path=CFG.project_root / \"data/amsterdam/train_with_image_paths.parquet\",\n",
    "#         image_path_col=\"image_path\",\n",
    "#         train_split_dir=CFG.data_dir / \"train\",\n",
    "#         val_split_dir=CFG.data_dir / \"val\",\n",
    "#         out_train_path=MET_DIR / \"metadata_train.parquet\",\n",
    "#         out_val_path=MET_DIR / \"metadata_val.parquet\",\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## Appendix C: blur + resize a dataset copy (optional)\n",
    "\n",
    "This refactors the original “create blurred + resized dataset” block into a function.\n",
    "\n",
    "- Applies a Gaussian blur (optional).\n",
    "- Resizes so the **shorter edge** equals `min_edge` (keeps aspect ratio).\n",
    "- Mirrors the ImageFolder structure (`split/class_name/image.jpg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "from PIL import Image, ImageFilter\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "IMG_EXTS = {\".jpg\", \".jpeg\", \".png\", \".bmp\", \".tif\", \".tiff\", \".webp\"}\n",
    "\n",
    "\n",
    "def iter_images(root: Path) -> Iterable[Path]:\n",
    "    root = Path(root)\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in IMG_EXTS:\n",
    "            yield p\n",
    "\n",
    "\n",
    "def blur_resize_imagefolder_copy(\n",
    "    src_dir: Path,\n",
    "    dst_dir: Path,\n",
    "    min_edge: int = 50,\n",
    "    blur_radius: float = 2.0,\n",
    "    jpg_quality: int = 95,\n",
    ") -> None:\n",
    "    '''\n",
    "    Create a processed copy of an ImageFolder dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_dir : Path\n",
    "        Source dataset root (contains train/val/test...).\n",
    "    dst_dir : Path\n",
    "        Destination root.\n",
    "    min_edge : int\n",
    "        Shorter side after resizing.\n",
    "    blur_radius : float\n",
    "        Gaussian blur radius. Use 0.0 to disable.\n",
    "    '''\n",
    "    src_dir = Path(src_dir)\n",
    "    dst_dir = Path(dst_dir)\n",
    "    resize = transforms.Resize(min_edge)\n",
    "\n",
    "    for img_path in iter_images(src_dir):\n",
    "        rel = img_path.relative_to(src_dir)\n",
    "        out_path = dst_dir / rel\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.convert(\"RGB\")\n",
    "                if blur_radius and blur_radius > 0:\n",
    "                    img = img.filter(ImageFilter.GaussianBlur(radius=float(blur_radius)))\n",
    "                img = resize(img)\n",
    "                img.save(out_path, quality=jpg_quality)\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {img_path}: {e}\")\n",
    "\n",
    "    print(\"Done.\")\n",
    "    print(\"Source      :\", src_dir)\n",
    "    print(\"Destination :\", dst_dir)\n",
    "\n",
    "\n",
    "# Example (disabled by default):\n",
    "# RUN_UTILS = True\n",
    "# if RUN_UTILS:\n",
    "#     blur_resize_imagefolder_copy(\n",
    "#         src_dir=CFG.data_dir,\n",
    "#         dst_dir=CFG.data_dir.parent / (CFG.data_dir.name + \"_processed\"),\n",
    "#         min_edge=50,\n",
    "#         blur_radius=2.0,\n",
    "#     )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis-py312)",
   "language": "python",
   "name": "thesis-py312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
